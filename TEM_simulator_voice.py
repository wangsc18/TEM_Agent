#!/usr/bin/env python3
"""
TEMåŒäººæ¨æ¼”æ¨¡æ‹Ÿå™¨ - è¯­éŸ³äº¤äº’ç‰ˆæœ¬
æ•´åˆå®æ—¶è¯­éŸ³è¯†åˆ«ã€LLMå¯¹è¯å’ŒTTSåŠŸèƒ½
"""
import tkinter as tk
from tkinter import messagebox, scrolledtext
import os
import time
import asyncio
import tempfile
import threading
from typing import Optional, Literal
import math

import numpy as np
import sounddevice as sd
from openai import AsyncOpenAI
from dotenv import load_dotenv

load_dotenv()

# --- æ¨¡æ‹Ÿçš„åå°æ•°æ® ---
MOCK_DATA = {
    "OFP": "é£è¡Œè®¡åˆ’ (OFP):\n\nèˆªçº¿: ZSSS -> ZBAA\né¢„è®¡æ²¹è€—: 15.2å¨\nå¤‡é™åœº: ZBTJ\nå·¡èˆªé«˜åº¦: FL350\nå¤‡æ³¨: ä¹˜å®¢ä¸­æœ‰åŒ»ç–—æ€¥æ•‘äººå‘˜ï¼Œéœ€å°½å¿«æŠµè¾¾ã€‚",
    "WEATHER": "æ°”è±¡æŠ¥å‘Š (METAR & TAF):\n\nZSSS (å‡ºå‘åœ°): 24015KT 9999 FEW030 25/18 Q1012 NOSIG\n\nZBAA (ç›®çš„åœ°): 20005KT 3000 BR SCT010 BKN020\nTAF ZBAA: ... TEMPO 0406 0500 FG BKN002\n(å¨èƒ: ç›®çš„åœ°æœºåœºæœ‰é›¾ï¼Œèƒ½è§åº¦å¯èƒ½åœ¨é¢„è®¡æŠµè¾¾æ—¶æ€¥å‰§ä¸‹é™è‡³500ç±³)",
    "TECH_LOG": "é£æœºæŠ€æœ¯æ—¥å¿—:\n\næ—¥æœŸ: 2025-10-26\né¡¹ç›®: APUï¼ˆè¾…åŠ©åŠ¨åŠ›å•å…ƒï¼‰å¯åŠ¨å‘ç”µæœºæ•…éšœ\nçŠ¶æ€: å·²æ ¹æ®MEL 49-11-01ä¿ç•™\nå½±å“: åœ°é¢æ— æ³•ä½¿ç”¨APUä¾›ç”µå’Œå¼•æ°”ï¼Œå¿…é¡»ä¾èµ–åœ°é¢è®¾å¤‡ã€‚",
    "NOTAMS": "èˆªè¡Œé€šå‘Š (NOTAMs):\n\nB3454/25 NOTAMN\nQ) ZSHA/QMRHW/IV/NBO/A/000/999/3114N12147E005\nA) ZSSS B) 2510250800 C) 2510251100\nE) RWY 17L/35R å› æ–½å·¥ï¼Œå¯ç”¨èµ·é£è·ç¦»ç¼©çŸ­400ç±³ã€‚\n(å¨èƒ: è·‘é“é•¿åº¦ç¼©çŸ­ï¼Œéœ€é‡æ–°è®¡ç®—èµ·é£æ€§èƒ½)",
}

# --- åŠ¨æ€äº‹ä»¶å®šä¹‰ ---
DYNAMIC_EVENT = {
    "title": "!! ç´§æ€¥é€šçŸ¥: æ¥è‡ªç­¾æ´¾ !!",
    "message": "æœ€æ–°æ¶ˆæ¯: æœºä¸Šå°†å¢åŠ ä¸€åéœ€è¦æ‹…æ¶çš„åŒ»ç–—æ—…å®¢åŠé™ªåŒå®¶å±ï¼Œæ€»é‡210å…¬æ–¤ã€‚è¯·ç«‹å³é‡æ–°è®¡ç®—é‡å¿ƒå’Œè½½é‡ï¼Œå¹¶è¯„ä¼°å¯¹èµ·é£æ€§èƒ½çš„å½±å“ã€‚",
}


# ============================================================================
# å¤´åƒåŠ¨ç”»ç»„ä»¶
# ============================================================================
class AvatarWidget(tk.Canvas):
    """åŠ¨æ€å¤´åƒç»„ä»¶ - æ”¯æŒè¯´è¯åŠ¨ç”»æ•ˆæœ"""

    def __init__(self, master, name: str, emoji: str, color: str, **kwargs):
        """
        åˆå§‹åŒ–å¤´åƒç»„ä»¶

        Args:
            name: åç§°ï¼ˆ"ä½ " æˆ– "AIä¼™ä¼´"ï¼‰
            emoji: è¡¨æƒ…ç¬¦å·ï¼ˆ"ğŸ‘¨â€âœˆï¸" æˆ– "ğŸ¤–"ï¼‰
            color: ä¸»é¢˜é¢œè‰²
        """
        super().__init__(master, width=120, height=140, bg="white", highlightthickness=0, **kwargs)

        self.name = name
        self.emoji = emoji
        self.color = color
        self.is_speaking = False
        self.animation_frame = 0
        self.animation_job = None

        # ç»˜åˆ¶é™æ€å…ƒç´ 
        self._draw_static()

    def _draw_static(self):
        """ç»˜åˆ¶é™æ€å…ƒç´ ï¼ˆå¤´åƒåœ†åœˆã€åç§°ï¼‰"""
        # æ¸…ç©ºç”»å¸ƒ
        self.delete("all")

        # å¤–åœˆï¼ˆç”¨äºåŠ¨ç”»ï¼‰
        self.outer_circle = self.create_oval(
            20, 20, 100, 100,
            outline=self.color,
            width=2,
            tags="outer"
        )

        # å†…åœˆï¼ˆå¤´åƒèƒŒæ™¯ï¼‰
        self.inner_circle = self.create_oval(
            30, 30, 90, 90,
            fill="#f0f0f0",
            outline=self.color,
            width=2,
            tags="inner"
        )

        # è¡¨æƒ…ç¬¦å·
        self.emoji_text = self.create_text(
            60, 60,
            text=self.emoji,
            font=("Arial", 32),
            tags="emoji"
        )

        # åç§°
        self.name_text = self.create_text(
            60, 115,
            text=self.name,
            font=("Helvetica", 11, "bold"),
            fill="#333"
        )

        # éŸ³é‡æ³¢å½¢æ¡ï¼ˆåˆå§‹éšè—ï¼‰
        self.wave_bars = []
        for i in range(5):
            bar = self.create_rectangle(
                15 + i * 22, 90,
                30 + i * 22, 95,
                fill=self.color,
                outline="",
                tags="wave",
                state="hidden"
            )
            self.wave_bars.append(bar)

    def start_speaking(self):
        """å¼€å§‹è¯´è¯åŠ¨ç”»"""
        if not self.is_speaking:
            self.is_speaking = True
            self.animation_frame = 0
            self._animate()

    def stop_speaking(self):
        """åœæ­¢è¯´è¯åŠ¨ç”»"""
        self.is_speaking = False
        if self.animation_job:
            self.after_cancel(self.animation_job)
            self.animation_job = None

        # æ¢å¤é™æ€çŠ¶æ€
        self._draw_static()

    def _animate(self):
        """åŠ¨ç”»å¾ªç¯"""
        if not self.is_speaking:
            return

        self.animation_frame += 1
        frame = self.animation_frame

        # 1. å¤–åœˆè„‰å†²æ•ˆæœï¼ˆç¼©æ”¾ï¼‰
        scale = 1.0 + 0.1 * math.sin(frame * 0.3)
        center = 60
        radius_outer = 40 * scale
        self.coords(
            self.outer_circle,
            center - radius_outer, center - radius_outer,
            center + radius_outer, center + radius_outer
        )

        # 2. å¤–åœˆé¢œè‰²å˜åŒ–
        color_rgb = self._hex_to_rgb(self.color)
        animated_color = f'#{color_rgb[0]:02x}{color_rgb[1]:02x}{color_rgb[2]:02x}'
        self.itemconfig(self.outer_circle, outline=animated_color, width=int(2 + 2 * math.sin(frame * 0.3)))

        # 3. éŸ³é‡æ³¢å½¢åŠ¨ç”»
        for i, bar in enumerate(self.wave_bars):
            # æ¯ä¸ªæŸ±å­ä¸åŒç›¸ä½
            height = 5 + 15 * abs(math.sin(frame * 0.2 + i * 0.5))
            self.coords(
                bar,
                15 + i * 22, 95 - height,
                30 + i * 22, 95
            )
            self.itemconfig(bar, state="normal")

        # 4. è¡¨æƒ…ç¬¦å·è½»å¾®è·³åŠ¨
        offset_y = 2 * math.sin(frame * 0.25)
        self.coords(self.emoji_text, 60, 60 + offset_y)

        # ç»§ç»­åŠ¨ç”»
        self.animation_job = self.after(50, self._animate)  # 20 FPS

    def _hex_to_rgb(self, hex_color: str) -> tuple:
        """å°†åå…­è¿›åˆ¶é¢œè‰²è½¬ä¸ºRGB"""
        hex_color = hex_color.lstrip('#')
        return tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))


# ============================================================================
# è¯­éŸ³äº¤äº’å¼•æ“ï¼ˆæ•´åˆè‡ª realtime_voice_agent_streaming.pyï¼‰
# ============================================================================
class VoiceInteractionEngine:
    """è¯­éŸ³äº¤äº’å¼•æ“ - ç”¨äºTEMæ¨¡æ‹Ÿå™¨"""

    def __init__(
        self,
        api_key: Optional[str] = None,
        model: str = "gpt-4o-mini",
        tts_engine: Literal["local", "edge", "openai"] = "local",
        callback_on_user_text=None,
        callback_on_ai_text=None,
        callback_on_ai_text_streaming=None,
        callback_on_status=None
    ):
        """
        åˆå§‹åŒ–è¯­éŸ³äº¤äº’å¼•æ“

        Args:
            callback_on_user_text: å½“è¯†åˆ«åˆ°ç”¨æˆ·è¯­éŸ³æ—¶çš„å›è°ƒ (user_text)
            callback_on_ai_text: å½“AIç”Ÿæˆå®Œæ•´å›å¤æ—¶çš„å›è°ƒ (ai_text)
            callback_on_ai_text_streaming: å½“AIæµå¼ç”Ÿæˆæ—¶çš„å›è°ƒ (partial_text)
            callback_on_status: çŠ¶æ€æ›´æ–°å›è°ƒ (status_text, status_type)
        """
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("æœªæ‰¾åˆ° OPENAI_API_KEY")

        self.client = AsyncOpenAI(api_key=self.api_key)
        self.model = model
        self.tts_engine = tts_engine

        self.callback_on_user_text = callback_on_user_text
        self.callback_on_ai_text = callback_on_ai_text
        self.callback_on_ai_text_streaming = callback_on_ai_text_streaming
        self.callback_on_status = callback_on_status

        # å½•éŸ³å‚æ•°
        self.sample_rate = 16000
        self.max_recording_duration = 10
        self.silence_threshold = 1.5
        self.silence_duration_to_stop = 0.02

        # å¯¹è¯å†å² - TEMåœºæ™¯ä¸“ç”¨prompt
        self.conversation_history = [
            {
                "role": "system",
                "content": """ä½ æ˜¯ä¸€åç»éªŒä¸°å¯Œçš„èˆªç©ºé£è¡Œå‘˜AIä¼™ä¼´ï¼Œæ­£åœ¨ä¸å¦ä¸€åé£è¡Œå‘˜è¿›è¡ŒTEMï¼ˆå¨èƒä¸å·®é”™ç®¡ç†ï¼‰æ¡ˆä¾‹è®¨è®ºã€‚

é‡è¦è¦æ±‚ï¼š
1. ç”¨å£è¯­åŒ–ã€è‡ªç„¶çš„æ–¹å¼äº¤æµï¼ŒåƒçœŸå®çš„é©¾é©¶èˆ±å¯¹è¯
2. ä½¿ç”¨èˆªç©ºä¸“ä¸šæœ¯è¯­ï¼Œä½†ä¿æŒå¯¹è¯æµç•…
3. ç§¯æè¯†åˆ«å¨èƒï¼ˆThreatsï¼‰ã€å·®é”™ï¼ˆErrorsï¼‰å’Œä¸è‰¯çŠ¶æ€ï¼ˆUndesired Statesï¼‰
4. æä¾›å»ºè®¾æ€§çš„å†³ç­–å»ºè®®
5. å¥å­ç®€çŸ­ï¼ˆ10-20å­—ï¼‰ï¼Œä¾¿äºè¯­éŸ³äº¤æµ
6. é€‚å½“ä½¿ç”¨"å—¯"ã€"å¥½çš„"ã€"æˆ‘è®¤ä¸º"ç­‰å£è¯­åŒ–è¡¨è¾¾

ç¤ºä¾‹ï¼š
âŒ ä¹¦é¢ï¼šæ ¹æ®å½“å‰æ°”è±¡æ¡ä»¶åˆ†æï¼Œæˆ‘ä»¬éœ€è¦åˆ¶å®šå¤‡é™æ–¹æ¡ˆã€‚
âœ… å£è¯­ï¼šå—¯ï¼Œçœ‹è¿™å¤©æ°”ï¼Œå’±ä»¬å¾—å‡†å¤‡å¥½å¤‡é™é¢„æ¡ˆå•Šã€‚

è®°ä½ï¼šç®€æ´ã€ä¸“ä¸šã€å£è¯­åŒ–ï¼"""
            }
        ]

        # ç”¨äºåœ¨åå°çº¿ç¨‹è¿è¡Œå¼‚æ­¥ä»»åŠ¡
        self.loop = None
        self.recording = False
        self.current_audio_data = []

    def _update_status(self, text: str, status_type: str = "info"):
        """æ›´æ–°çŠ¶æ€ï¼ˆåœ¨ä¸»çº¿ç¨‹è°ƒç”¨å›è°ƒï¼‰"""
        if self.callback_on_status:
            self.callback_on_status(text, status_type)

    def _on_user_text_recognized(self, text: str):
        """ç”¨æˆ·è¯­éŸ³è¯†åˆ«å®Œæˆ"""
        if self.callback_on_user_text:
            self.callback_on_user_text(text)

    def _on_ai_response(self, text: str):
        """AIå›å¤ç”Ÿæˆå®Œæˆ"""
        if self.callback_on_ai_text:
            self.callback_on_ai_text(text)

    def _on_ai_response_streaming(self, text: str):
        """AIæµå¼ç”Ÿæˆä¸­ï¼ˆæ¯ç”Ÿæˆä¸€ä¸ªå¥å­è°ƒç”¨ï¼‰"""
        if self.callback_on_ai_text_streaming:
            self.callback_on_ai_text_streaming(text)

    def start_recording(self):
        """å¼€å§‹å½•éŸ³ï¼ˆä»…å½•éŸ³+STTï¼Œä¸è§¦å‘LLMï¼‰"""
        def run_async_recording():
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            self.loop = loop
            loop.run_until_complete(self._async_record_and_transcribe())

        thread = threading.Thread(target=run_async_recording, daemon=True)
        thread.start()

    async def _async_record_and_transcribe(self):
        """å¼‚æ­¥å½•éŸ³å¹¶è½¬æ–‡å­—ï¼ˆä»…STTï¼Œä¸è°ƒç”¨LLMï¼‰"""
        try:
            # 1. å½•éŸ³
            self._update_status("ğŸ¤ æ­£åœ¨å½•éŸ³...", "recording")
            audio_data = await self._record_audio()

            if audio_data is None:
                self._update_status("âŒ å½•éŸ³å¤±è´¥", "error")
                return

            # 2. è¯­éŸ³è¯†åˆ«
            self._update_status("ğŸ”„ è¯­éŸ³è¯†åˆ«ä¸­...", "processing")
            user_text = await self._speech_to_text(audio_data)

            if not user_text:
                self._update_status("âŒ æœªè¯†åˆ«åˆ°è¯­éŸ³", "error")
                return

            # 3. å°†è¯†åˆ«ç»“æœå¡«å……åˆ°è¾“å…¥æ¡†ï¼ˆä¸è‡ªåŠ¨å‘é€ï¼‰
            self._on_user_text_recognized(user_text)
            self._update_status("âœ“ è¯†åˆ«å®Œæˆï¼Œè¯·ç¡®è®¤åå‘é€", "success")

        except Exception as e:
            self._update_status(f"âŒ é”™è¯¯: {str(e)}", "error")

    def process_user_message(self, user_message: str):
        """å¤„ç†ç”¨æˆ·æ¶ˆæ¯ï¼ˆLLMå¯¹è¯+TTSï¼‰"""
        def run_async_processing():
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            self.loop = loop
            loop.run_until_complete(self._async_llm_and_tts(user_message))

        thread = threading.Thread(target=run_async_processing, daemon=True)
        thread.start()

    async def _async_llm_and_tts(self, user_message: str):
        """å¼‚æ­¥LLMç”Ÿæˆå’ŒTTSæ’­æ”¾"""
        try:
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
            self.conversation_history.append({
                "role": "user",
                "content": user_message
            })

            # 1. å¼€å§‹æµå¼LLMç”Ÿæˆ
            self._update_status("ğŸ¤– AIæ€è€ƒä¸­...", "processing")

            stream = await self.client.chat.completions.create(
                model=self.model,
                messages=self.conversation_history,
                stream=True,
                temperature=0.7,
                max_tokens=300
            )

            # 2. æµå¼å¤„ç†ï¼šè¾¹ç”Ÿæˆè¾¹TTS
            full_response = ""
            current_chunk = ""
            audio_queue = []  # å­˜å‚¨å¾…æ’­æ”¾çš„éŸ³é¢‘æ–‡ä»¶

            # å¯åŠ¨éŸ³é¢‘æ’­æ”¾åç¨‹
            play_task = asyncio.create_task(self._audio_player(audio_queue))

            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    token = chunk.choices[0].delta.content
                    current_chunk += token
                    full_response += token

                    # æ£€æµ‹æ˜¯å¦æœ‰å®Œæ•´çš„å¥å­/çŸ­è¯­
                    sentences = self._extract_complete_sentences(current_chunk)

                    if sentences:
                        for sentence in sentences:
                            # æµå¼æ›´æ–°æ˜¾ç¤ºï¼ˆæ¯ç”Ÿæˆä¸€ä¸ªå¥å­å°±æ˜¾ç¤ºï¼‰
                            self._on_ai_response_streaming(sentence)

                            # ç«‹å³è¿›è¡ŒTTS
                            audio_file = await self._quick_tts(sentence)
                            if audio_file:
                                audio_queue.append(audio_file)

                                # é¦–æ¬¡æ’­æ”¾æ—¶æ›´æ–°çŠ¶æ€
                                if len(audio_queue) == 1:
                                    self._update_status("ğŸ”Š æ’­æ”¾AIè¯­éŸ³...", "speaking")

                        # é‡ç½®ç¼“å†²åŒºï¼ˆä¿ç•™æœªå®Œæˆçš„éƒ¨åˆ†ï¼‰
                        current_chunk = self._get_remaining_text(current_chunk, sentences)

            # å¤„ç†æœ€åå‰©ä½™çš„æ–‡æœ¬
            if current_chunk.strip():
                # æµå¼æ›´æ–°æ˜¾ç¤ºæœ€åä¸€æ®µ
                self._on_ai_response_streaming(current_chunk.strip())

                audio_file = await self._quick_tts(current_chunk.strip())
                if audio_file:
                    audio_queue.append(audio_file)

            # æ ‡è®°éŸ³é¢‘é˜Ÿåˆ—ç»“æŸ
            audio_queue.append(None)  # ç»“æŸä¿¡å·

            # ç­‰å¾…æ‰€æœ‰éŸ³é¢‘æ’­æ”¾å®Œæˆ
            await play_task

            # æ·»åŠ AIå›å¤åˆ°å†å²
            self.conversation_history.append({
                "role": "assistant",
                "content": full_response
            })

            # å›è°ƒæ˜¾ç¤ºå®Œæ•´å›å¤
            self._on_ai_response(full_response.strip())

            self._update_status("âœ“ å®Œæˆ", "success")

        except Exception as e:
            self._update_status(f"âŒ é”™è¯¯: {str(e)}", "error")
            print(f"æµå¼LLM+TTSé”™è¯¯: {e}")

    def _extract_complete_sentences(self, text: str) -> list:
        """æå–å®Œæ•´çš„å¥å­ï¼ˆæŒ‰æ ‡ç‚¹ç¬¦å·åˆ†å‰²ï¼‰"""
        import re
        # åŒ¹é…ä¸­è‹±æ–‡æ ‡ç‚¹
        pattern = r'([^ï¼Œã€‚ï¼ï¼Ÿ,\.!?]+[ï¼Œã€‚ï¼ï¼Ÿ,\.!?]+)'
        matches = re.findall(pattern, text)
        return [m.strip() for m in matches if m.strip()]

    def _get_remaining_text(self, text: str, extracted_sentences: list) -> str:
        """è·å–æå–å¥å­åå‰©ä½™çš„æ–‡æœ¬"""
        for sentence in extracted_sentences:
            text = text.replace(sentence, '', 1)
        return text

    async def _quick_tts(self, text: str) -> Optional[str]:
        """å¿«é€ŸTTSï¼ˆå•ä¸ªå¥å­/çŸ­è¯­ï¼‰"""
        try:
            if self.tts_engine == "local":
                # macOS sayå‘½ä»¤ï¼ˆæœ€å¿«ï¼‰
                temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".m4a")
                temp_file.close()

                process = await asyncio.create_subprocess_exec(
                    "say",
                    "-v", "Tingting",
                    "-o", temp_file.name,
                    "--data-format=LEF32@22050",
                    text,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                await process.communicate()

                if process.returncode == 0 and os.path.exists(temp_file.name):
                    return temp_file.name
                else:
                    if os.path.exists(temp_file.name):
                        os.unlink(temp_file.name)
                    return None

            elif self.tts_engine == "edge":
                # Edge TTS
                temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
                temp_file.close()

                process = await asyncio.create_subprocess_exec(
                    "edge-tts",
                    "--voice", "zh-CN-XiaoxiaoNeural",
                    "--rate", "+10%",
                    "--pitch", "+5Hz",
                    "--text", text,
                    "--write-media", temp_file.name,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                await process.communicate()

                if process.returncode == 0 and os.path.exists(temp_file.name):
                    return temp_file.name
                else:
                    if os.path.exists(temp_file.name):
                        os.unlink(temp_file.name)
                    return None

            else:  # openai
                # OpenAI TTSï¼ˆè¾ƒæ…¢ï¼Œä¸æ¨èæµå¼ä½¿ç”¨ï¼‰
                response = await self.client.audio.speech.create(
                    model="tts-1",
                    voice="nova",
                    input=text
                )

                temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
                temp_file.write(response.content)
                temp_file.close()
                return temp_file.name

        except Exception as e:
            print(f"å¿«é€ŸTTSé”™è¯¯: {e}")
            return None

    async def _audio_player(self, audio_queue: list):
        """éŸ³é¢‘æ’­æ”¾å™¨ï¼ˆå¹¶å‘æ’­æ”¾é˜Ÿåˆ—ä¸­çš„éŸ³é¢‘ï¼‰"""
        try:
            while True:
                # ç­‰å¾…é˜Ÿåˆ—ä¸­æœ‰éŸ³é¢‘
                while len(audio_queue) == 0:
                    await asyncio.sleep(0.1)

                # å–å‡ºéŸ³é¢‘æ–‡ä»¶
                audio_file = audio_queue.pop(0)

                # Noneè¡¨ç¤ºé˜Ÿåˆ—ç»“æŸ
                if audio_file is None:
                    break

                # æ’­æ”¾éŸ³é¢‘
                if os.path.exists(audio_file):
                    play_process = await asyncio.create_subprocess_exec(
                        "afplay", audio_file,
                        stdout=asyncio.subprocess.PIPE,
                        stderr=asyncio.subprocess.PIPE
                    )
                    await play_process.communicate()

                    # æ’­æ”¾å®Œæˆååˆ é™¤ä¸´æ—¶æ–‡ä»¶
                    try:
                        os.unlink(audio_file)
                    except:
                        pass

        except Exception as e:
            print(f"éŸ³é¢‘æ’­æ”¾å™¨é”™è¯¯: {e}")

    async def _record_audio(self) -> Optional[np.ndarray]:
        """å½•éŸ³ï¼ˆç®€åŒ–ç‰ˆï¼Œè‡ªåŠ¨é™éŸ³æ£€æµ‹ï¼‰"""
        try:
            audio_chunks = []
            silence_start_time = None

            def audio_callback(indata, _frames, _time_info, status):
                if status:
                    print(f"å½•éŸ³çŠ¶æ€: {status}")
                audio_chunks.append(indata.copy())

            with sd.InputStream(
                samplerate=self.sample_rate,
                channels=1,
                dtype=np.float32,
                callback=audio_callback
            ):
                start_time = time.time()
                while time.time() - start_time < self.max_recording_duration:
                    await asyncio.sleep(0.1)

                    if len(audio_chunks) > 0:
                        recent_audio = np.concatenate(audio_chunks[-5:])
                        rms = np.sqrt(np.mean(recent_audio ** 2))

                        if rms < self.silence_duration_to_stop:
                            if silence_start_time is None:
                                silence_start_time = time.time()
                            elif time.time() - silence_start_time > self.silence_threshold:
                                break
                        else:
                            silence_start_time = None

            if len(audio_chunks) == 0:
                return None

            audio_data = np.concatenate(audio_chunks)
            return audio_data

        except Exception as e:
            print(f"å½•éŸ³é”™è¯¯: {e}")
            return None

    async def _speech_to_text(self, audio_data: np.ndarray) -> str:
        """è¯­éŸ³è½¬æ–‡å­—"""
        try:
            # ä¿å­˜ä¸ºä¸´æ—¶WAVæ–‡ä»¶
            import wave
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")

            with wave.open(temp_file.name, 'wb') as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(self.sample_rate)
                audio_int16 = (audio_data * 32767).astype(np.int16)
                wf.writeframes(audio_int16.tobytes())

            # è°ƒç”¨Whisper API
            with open(temp_file.name, 'rb') as audio_file:
                transcription = await self.client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file,
                    language="zh"
                )

            os.unlink(temp_file.name)
            return transcription.text.strip()

        except Exception as e:
            print(f"STTé”™è¯¯: {e}")
            return ""

    async def _get_llm_response(self, user_message: str) -> str:
        """è·å–LLMå›å¤"""
        try:
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
            self.conversation_history.append({
                "role": "user",
                "content": user_message
            })

            # è°ƒç”¨LLMï¼ˆæµå¼ï¼‰
            full_response = ""
            stream = await self.client.chat.completions.create(
                model=self.model,
                messages=self.conversation_history,
                stream=True,
                temperature=0.7,
                max_tokens=300
            )

            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    full_response += chunk.choices[0].delta.content

            # æ·»åŠ AIå›å¤åˆ°å†å²
            self.conversation_history.append({
                "role": "assistant",
                "content": full_response
            })

            return full_response.strip()

        except Exception as e:
            print(f"LLMé”™è¯¯: {e}")
            return ""

    async def _text_to_speech_and_play(self, text: str):
        """æ–‡å­—è½¬è¯­éŸ³å¹¶æ’­æ”¾"""
        try:
            if self.tts_engine == "local":
                # macOS sayå‘½ä»¤
                temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".m4a")
                temp_file.close()

                process = await asyncio.create_subprocess_exec(
                    "say",
                    "-v", "Tingting",
                    "-o", temp_file.name,
                    "--data-format=LEF32@22050",
                    text,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                await process.communicate()

                if process.returncode == 0 and os.path.exists(temp_file.name):
                    # æ’­æ”¾éŸ³é¢‘
                    play_process = await asyncio.create_subprocess_exec(
                        "afplay", temp_file.name,
                        stdout=asyncio.subprocess.PIPE,
                        stderr=asyncio.subprocess.PIPE
                    )
                    await play_process.communicate()
                    os.unlink(temp_file.name)

            elif self.tts_engine == "edge":
                # Edge TTS
                temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
                temp_file.close()

                process = await asyncio.create_subprocess_exec(
                    "edge-tts",
                    "--voice", "zh-CN-XiaoxiaoNeural",
                    "--rate", "+10%",
                    "--pitch", "+5Hz",
                    "--text", text,
                    "--write-media", temp_file.name,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                await process.communicate()

                if process.returncode == 0 and os.path.exists(temp_file.name):
                    play_process = await asyncio.create_subprocess_exec(
                        "afplay", temp_file.name,
                        stdout=asyncio.subprocess.PIPE,
                        stderr=asyncio.subprocess.PIPE
                    )
                    await play_process.communicate()
                    os.unlink(temp_file.name)

            else:  # openai
                # OpenAI TTS
                response = await self.client.audio.speech.create(
                    model="tts-1",
                    voice="nova",
                    input=text
                )

                temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
                temp_file.write(response.content)
                temp_file.close()

                play_process = await asyncio.create_subprocess_exec(
                    "afplay", temp_file.name,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                await play_process.communicate()
                os.unlink(temp_file.name)

        except Exception as e:
            print(f"TTSé”™è¯¯: {e}")


# ============================================================================
# TEMæ¨¡æ‹Ÿå™¨ä¸»åº”ç”¨
# ============================================================================
class TEMSimulatorApp:
    """ä¸»åº”ç”¨æ§åˆ¶å™¨"""
    def __init__(self, root):
        self.root = root
        self.root.title("TEMåŒäººæ¨æ¼”æ¨¡æ‹Ÿå™¨ - è¯­éŸ³äº¤äº’ç‰ˆ")
        self.root.geometry("1200x800")

        self.current_phase = "INDIVIDUAL"

        # åˆå§‹åŒ–è¯­éŸ³å¼•æ“
        self.voice_engine = None
        self._init_voice_engine()

        # --- UI é¢æ¿åˆå§‹åŒ– ---
        self.root.grid_rowconfigure(0, weight=1)
        self.root.grid_columnconfigure(0, weight=1)
        self.root.grid_columnconfigure(1, weight=3)
        self.root.grid_columnconfigure(2, weight=2)

        self.left_panel = LeftPanel(self.root, self)
        self.center_panel = CenterPanel(self.root, self)
        self.right_panel = RightPanel(self.root, self)

        self.left_panel.grid(row=0, column=0, sticky="nsew", padx=5, pady=5)
        self.center_panel.grid(row=0, column=1, sticky="nsew", padx=5, pady=5)
        self.right_panel.grid(row=0, column=2, sticky="nsew", padx=5, pady=5)

    def _init_voice_engine(self):
        """åˆå§‹åŒ–è¯­éŸ³å¼•æ“"""
        try:
            self.voice_engine = VoiceInteractionEngine(
                tts_engine="edge",  # ä½¿ç”¨edgeTTSï¼Œç»¼åˆé€Ÿåº¦å’Œè´¨é‡
                callback_on_user_text=self._on_user_speech_recognized,
                callback_on_ai_text=self._on_ai_speech_generated,
                callback_on_ai_text_streaming=self._on_ai_speech_streaming,
                callback_on_status=self._on_voice_status_update
            )
            print("[è¯­éŸ³å¼•æ“] åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"[è¯­éŸ³å¼•æ“] åˆå§‹åŒ–å¤±è´¥: {e}")
            messagebox.showerror("é”™è¯¯", f"è¯­éŸ³å¼•æ“åˆå§‹åŒ–å¤±è´¥ï¼š{str(e)}\nè¯·æ£€æŸ¥.envä¸­çš„OPENAI_API_KEY")

    def _on_user_speech_recognized(self, text: str):
        """ç”¨æˆ·è¯­éŸ³è¯†åˆ«å®Œæˆçš„å›è°ƒ - å¡«å……åˆ°è¾“å…¥æ¡†"""
        # åœ¨ä¸»çº¿ç¨‹å¡«å……è¾“å…¥æ¡†
        self.root.after(0, lambda: self.right_panel.fill_input_from_voice(text))

    def _on_ai_speech_streaming(self, text: str):
        """AIæµå¼ç”Ÿæˆå›è°ƒ - å®æ—¶æ›´æ–°æ˜¾ç¤º"""
        # åœ¨ä¸»çº¿ç¨‹æµå¼æ›´æ–°å¯¹è¯æ¡†
        self.root.after(0, lambda: self.right_panel.append_ai_message_streaming(text))

    def _on_ai_speech_generated(self, _text: str):
        """AIå›å¤ç”Ÿæˆå®Œæˆçš„å›è°ƒ"""
        # æµå¼æ¨¡å¼ä¸‹ä¸éœ€è¦è¿™ä¸ªå›è°ƒï¼ˆå·²é€šè¿‡streamingæ›´æ–°ï¼‰
        pass

    def _on_voice_status_update(self, status_text: str, status_type: str):
        """è¯­éŸ³çŠ¶æ€æ›´æ–°å›è°ƒ"""
        # åœ¨ä¸»çº¿ç¨‹æ›´æ–°çŠ¶æ€æ˜¾ç¤ºå’Œå¤´åƒåŠ¨ç”»
        def update_ui():
            self.right_panel.update_voice_status(status_text, status_type)

            # æ§åˆ¶å¤´åƒåŠ¨ç”»
            if status_type == "recording":
                # ç”¨æˆ·æ­£åœ¨è¯´è¯
                self.right_panel.user_avatar.start_speaking()
                self.right_panel.ai_avatar.stop_speaking()
            elif status_type == "speaking":
                # AIæ­£åœ¨è¯´è¯
                self.right_panel.user_avatar.stop_speaking()
                self.right_panel.ai_avatar.start_speaking()
            elif status_type in ["success", "error"]:
                # å®Œæˆæˆ–é”™è¯¯ï¼Œåœæ­¢æ‰€æœ‰åŠ¨ç”»
                self.right_panel.user_avatar.stop_speaking()
                self.right_panel.ai_avatar.stop_speaking()
                # ç»“æŸAIæµå¼æ˜¾ç¤º
                if status_type == "success":
                    self.right_panel._end_ai_streaming()

        self.root.after(0, update_ui)

    def on_voice_input_button_click(self):
        """è¯­éŸ³è¾“å…¥æŒ‰é’®è¢«ç‚¹å‡»"""
        if self.voice_engine:
            self.voice_engine.start_recording()
        else:
            messagebox.showwarning("è­¦å‘Š", "è¯­éŸ³å¼•æ“æœªåˆå§‹åŒ–")

    def on_info_button_click(self, info_type):
        """å½“å·¦ä¾§ä¿¡æ¯æŒ‰é’®è¢«ç‚¹å‡»æ—¶"""
        print(f"[äº‹ä»¶] ç”¨æˆ·è¯·æ±‚æŸ¥çœ‹ '{info_type}'")
        data_to_display = MOCK_DATA.get(info_type, "æœªæ‰¾åˆ°ä¿¡æ¯ã€‚")
        self.center_panel.display_info(info_type, data_to_display)

    def start_team_discussion(self):
        """åˆ‡æ¢åˆ°åŒäººåä½œé˜¶æ®µ"""
        if self.current_phase == "INDIVIDUAL":
            print("[äº‹ä»¶] åˆ‡æ¢åˆ°åä½œè®¨è®ºé˜¶æ®µã€‚")

            # 1. ä¿å­˜ä¸ªäººå¨èƒå¤‡å¿˜å½•å†…å®¹
            personal_threats = self.right_panel.get_personal_threats()

            # 2. åˆ‡æ¢åˆ°åä½œé˜¶æ®µ
            self.current_phase = "COLLABORATIVE"
            self.right_panel.setup_collaborative_view()
            self.left_panel.disable_buttons()

            # 3. åœ¨ä¸­é—´é¢æ¿æ˜¾ç¤ºä¸ªäººå¨èƒæ€»ç»“
            self.center_panel.display_personal_threats(personal_threats)

            # 4. 3ç§’åæ³¨å…¥åŠ¨æ€äº‹ä»¶
            self.root.after(3000, self.inject_dynamic_event)

    def inject_dynamic_event(self):
        """æ³¨å…¥åŠ¨æ€äº‹ä»¶"""
        print("[äº‹ä»¶] æ³¨å…¥åŠ¨æ€äº‹ä»¶ï¼")
        messagebox.showwarning(DYNAMIC_EVENT["title"], DYNAMIC_EVENT["message"])


# ============================================================================
# UIé¢æ¿ç»„ä»¶
# ============================================================================
class LeftPanel(tk.Frame):
    """å·¦ä¾§å¯¼èˆªæ """
    def __init__(self, master, controller):
        super().__init__(master, bd=2, relief=tk.SUNKEN)
        self.controller = controller

        tk.Label(self, text="ä¿¡æ¯æº", font=("Helvetica", 14, "bold")).pack(pady=10)

        self.buttons = {}
        info_types = {"OFP": "é£è¡Œè®¡åˆ’", "WEATHER": "å¤©æ°”", "TECH_LOG": "æŠ€æœ¯æ—¥å¿—", "NOTAMS": "èˆªè¡Œé€šå‘Š"}
        for key, text in info_types.items():
            btn = tk.Button(self, text=text, command=lambda k=key: self.controller.on_info_button_click(k))
            btn.pack(fill=tk.X, padx=10, pady=5)
            self.buttons[key] = btn

    def disable_buttons(self):
        for btn in self.buttons.values():
            btn.config(state=tk.DISABLED)


class CenterPanel(tk.Frame):
    """ä¸­é—´ä¿¡æ¯æ˜¾ç¤ºåŒº"""
    def __init__(self, master, controller):
        super().__init__(master, bd=2, relief=tk.SUNKEN)
        self.controller = controller

        self.title_label = tk.Label(self, text="è¯·ä»å·¦ä¾§é€‰æ‹©ä¿¡æ¯æº", font=("Helvetica", 14, "bold"))
        self.title_label.pack(pady=10)

        self.text_area = scrolledtext.ScrolledText(self, wrap=tk.WORD, font=("Helvetica", 12))
        self.text_area.pack(expand=True, fill=tk.BOTH, padx=10, pady=10)
        self.text_area.config(state=tk.DISABLED)

    def display_info(self, title, content):
        self.title_label.config(text=title)
        self.text_area.config(state=tk.NORMAL)
        self.text_area.delete('1.0', tk.END)
        self.text_area.insert(tk.END, content)
        self.text_area.config(state=tk.DISABLED)

    def display_personal_threats(self, threats_content: str):
        """æ˜¾ç¤ºä¸ªäººå¨èƒæ€»ç»“ï¼ˆé˜¶æ®µäºŒï¼‰"""
        self.title_label.config(text="ğŸ“‹ ä¸ªäººå¨èƒæ€»ç»“", fg="#FF5722")
        self.text_area.config(state=tk.NORMAL)
        self.text_area.delete('1.0', tk.END)

        if threats_content.strip():
            self.text_area.insert(tk.END, "ä»¥ä¸‹æ˜¯ä½ åœ¨ä¸ªäººä¿¡æ¯æ”¶é›†é˜¶æ®µæ€»ç»“çš„æ½œåœ¨å¨èƒï¼š\n\n")
            self.text_area.insert(tk.END, "="*50 + "\n\n")
            self.text_area.insert(tk.END, threats_content)
        else:
            self.text_area.insert(tk.END, "ï¼ˆä½ åœ¨é˜¶æ®µä¸€æ²¡æœ‰è®°å½•ä»»ä½•å¨èƒï¼‰\n\n")
            self.text_area.insert(tk.END, "å»ºè®®ï¼šåœ¨å›¢é˜Ÿè®¨è®ºä¸­ï¼Œå¯ä»¥å›é¡¾å·¦ä¾§ä¿¡æ¯æºï¼Œè¯†åˆ«æ–°çš„å¨èƒã€‚")

        self.text_area.config(state=tk.DISABLED)


class RightPanel(tk.Frame):
    """å³ä¾§åä½œä¸å†³ç­–åŒº"""
    def __init__(self, master, controller):
        super().__init__(master, bd=2, relief=tk.SUNKEN)
        self.controller = controller
        self.setup_individual_view()

    def clear_panel(self):
        for widget in self.winfo_children():
            widget.destroy()

    def get_personal_threats(self) -> str:
        """è·å–ä¸ªäººå¨èƒå¤‡å¿˜å½•å†…å®¹ï¼ˆåœ¨åˆ‡æ¢é˜¶æ®µå‰è°ƒç”¨ï¼‰"""
        if hasattr(self, 'memo_area'):
            return self.memo_area.get('1.0', tk.END).strip()
        return ""

    def setup_individual_view(self):
        """è®¾ç½®ä¸ªäººä¿¡æ¯æ”¶é›†é˜¶æ®µçš„ç•Œé¢"""
        self.clear_panel()
        tk.Label(self, text="ä¸ªäººå¨èƒå¤‡å¿˜å½•", font=("Helvetica", 14, "bold")).pack(pady=10)

        self.memo_area = scrolledtext.ScrolledText(self, wrap=tk.WORD, height=10)
        self.memo_area.pack(expand=True, fill=tk.BOTH, padx=10, pady=5)

        tk.Button(self, text="è¿›å…¥å›¢é˜Ÿè®¨è®º >>", command=self.controller.start_team_discussion).pack(pady=10)

    def setup_collaborative_view(self):
        """è®¾ç½®åŒäººè®¨è®ºé˜¶æ®µçš„ç•Œé¢ï¼ˆå¸¦è¯­éŸ³äº¤äº’ï¼‰"""
        self.clear_panel()

        # å¤´åƒæ˜¾ç¤ºåŒº
        avatar_frame = tk.Frame(self, bg="white")
        avatar_frame.pack(fill=tk.X, padx=10, pady=10)

        # ç”¨æˆ·å¤´åƒï¼ˆå·¦ä¾§ï¼‰
        self.user_avatar = AvatarWidget(
            avatar_frame,
            name="ä½ ",
            emoji="ğŸ‘¨â€âœˆï¸",
            color="#2196F3"  # è“è‰²
        )
        self.user_avatar.pack(side=tk.LEFT, padx=10)

        # AIå¤´åƒï¼ˆå³ä¾§ï¼‰
        self.ai_avatar = AvatarWidget(
            avatar_frame,
            name="AIä¼™ä¼´",
            emoji="ğŸ¤–",
            color="#4CAF50"  # ç»¿è‰²
        )
        self.ai_avatar.pack(side=tk.RIGHT, padx=10)

        # åˆ†éš”çº¿
        tk.Frame(self, height=2, bg="#e0e0e0").pack(fill=tk.X, padx=10, pady=5)

        # å¨èƒæ—¥å¿—ï¼ˆç¼©å°ï¼‰
        tk.Label(self, text="å›¢é˜Ÿå¨èƒæ—¥å¿—", font=("Helvetica", 11, "bold")).pack(pady=3)
        self.threat_log = tk.Listbox(self, height=4)
        self.threat_log.pack(fill=tk.X, padx=10, pady=3)

        # å¯¹è¯åŒºï¼ˆç¼©å°é«˜åº¦ï¼‰
        tk.Label(self, text="å›¢é˜Ÿé€šè®¯", font=("Helvetica", 11, "bold")).pack(pady=3)
        self.chat_area = scrolledtext.ScrolledText(self, wrap=tk.WORD, height=6)
        self.chat_area.pack(fill=tk.BOTH, padx=10, pady=3)
        self.chat_area.config(state=tk.DISABLED)

        # è¯­éŸ³äº¤äº’æ§åˆ¶åŒº
        voice_frame = tk.LabelFrame(self, text="ğŸ¤ è¯­éŸ³äº¤äº’", font=("Helvetica", 11, "bold"))
        voice_frame.pack(fill=tk.X, padx=10, pady=10)

        # è¯­éŸ³è¾“å…¥æŒ‰é’®
        self.voice_button = tk.Button(
            voice_frame,
            text="ğŸ¤ è¯­éŸ³è¾“å…¥",
            font=("Helvetica", 12, "bold"),
            bg="#4CAF50",
            fg="white",
            height=2,
            command=self.controller.on_voice_input_button_click
        )
        self.voice_button.pack(fill=tk.X, padx=10, pady=5)

        # çŠ¶æ€æ˜¾ç¤º
        self.status_label = tk.Label(
            voice_frame,
            text="ç‚¹å‡»ä¸Šæ–¹æŒ‰é’®å¼€å§‹è¯­éŸ³è¾“å…¥",
            font=("Helvetica", 10),
            fg="gray"
        )
        self.status_label.pack(pady=5)

        # æ–‡å­—è¾“å…¥åŒº
        text_input_frame = tk.Frame(voice_frame)
        text_input_frame.pack(fill=tk.X, padx=10, pady=5)

        tk.Label(text_input_frame, text="æ¶ˆæ¯è¾“å…¥:", font=("Helvetica", 9)).pack(anchor=tk.W)

        entry_frame = tk.Frame(text_input_frame)
        entry_frame.pack(fill=tk.X)

        self.chat_entry = tk.Entry(entry_frame, font=("Helvetica", 10))
        self.chat_entry.pack(side=tk.LEFT, expand=True, fill=tk.X)
        self.chat_entry.bind("<Return>", lambda _event: self.send_text_message())

        tk.Button(entry_frame, text="å‘é€", command=self.send_text_message).pack(side=tk.RIGHT, padx=(5, 0))

    def fill_input_from_voice(self, text: str):
        """ä»è¯­éŸ³è¯†åˆ«ç»“æœå¡«å……è¾“å…¥æ¡†"""
        self.chat_entry.delete(0, tk.END)
        self.chat_entry.insert(0, text)
        self.chat_entry.focus_set()  # èšç„¦åˆ°è¾“å…¥æ¡†ï¼Œæ–¹ä¾¿ç”¨æˆ·ä¿®æ”¹

    def send_text_message(self):
        """å‘é€æ–‡å­—æ¶ˆæ¯ - è§¦å‘LLMå¯¹è¯"""
        message = self.chat_entry.get()
        if message.strip():
            # 1. æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å¯¹è¯æ¡†
            self.add_chat_message("ä½ ", message)
            self.chat_entry.delete(0, tk.END)

            # 2. å‡†å¤‡æ¥æ”¶AIæµå¼å›å¤
            self._start_ai_streaming()

            # 3. è§¦å‘è¯­éŸ³å¼•æ“å¤„ç†ï¼ˆLLM + TTSï¼‰
            if self.controller.voice_engine:
                self.controller.voice_engine.process_user_message(message)

    def _start_ai_streaming(self):
        """å¼€å§‹AIæµå¼å›å¤ï¼ˆåˆå§‹åŒ–çŠ¶æ€ï¼‰"""
        self.chat_area.config(state=tk.NORMAL)
        self.chat_area.insert(tk.END, "AIä¼™ä¼´: ")
        self.chat_area.yview(tk.END)
        self.chat_area.config(state=tk.DISABLED)
        self._ai_streaming_active = True

    def append_ai_message_streaming(self, text: str):
        """æµå¼è¿½åŠ AIæ¶ˆæ¯å†…å®¹"""
        self.chat_area.config(state=tk.NORMAL)
        self.chat_area.insert(tk.END, text)
        self.chat_area.yview(tk.END)
        self.chat_area.config(state=tk.DISABLED)

    def _end_ai_streaming(self):
        """ç»“æŸAIæµå¼å›å¤"""
        if hasattr(self, '_ai_streaming_active') and self._ai_streaming_active:
            self.chat_area.config(state=tk.NORMAL)
            self.chat_area.insert(tk.END, "\n")
            self.chat_area.yview(tk.END)
            self.chat_area.config(state=tk.DISABLED)
            self._ai_streaming_active = False

    def add_chat_message(self, author, message):
        """æ·»åŠ èŠå¤©æ¶ˆæ¯"""
        self.chat_area.config(state=tk.NORMAL)
        self.chat_area.insert(tk.END, f"{author}: {message}\n")
        self.chat_area.yview(tk.END)
        self.chat_area.config(state=tk.DISABLED)

    def update_voice_status(self, status_text: str, status_type: str):
        """æ›´æ–°è¯­éŸ³çŠ¶æ€æ˜¾ç¤º"""
        color_map = {
            "recording": "#FF5722",  # çº¢è‰²-å½•éŸ³ä¸­
            "processing": "#2196F3",  # è“è‰²-å¤„ç†ä¸­
            "speaking": "#4CAF50",    # ç»¿è‰²-æ’­æ”¾ä¸­
            "success": "#4CAF50",     # ç»¿è‰²-æˆåŠŸ
            "error": "#F44336",       # çº¢è‰²-é”™è¯¯
            "info": "gray"            # ç°è‰²-ä¿¡æ¯
        }

        self.status_label.config(
            text=status_text,
            fg=color_map.get(status_type, "gray")
        )


# ============================================================================
# ä¸»ç¨‹åºå…¥å£
# ============================================================================
if __name__ == "__main__":
    root = tk.Tk()
    app = TEMSimulatorApp(root)
    root.mainloop()
