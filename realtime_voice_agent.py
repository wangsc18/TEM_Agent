#!/usr/bin/env python3
"""
Real-time Voice Interaction Agent
ä½¿ç”¨ OpenAI API å®ç°å®æ—¶è¯­éŸ³äº¤äº’ï¼ŒåŒ…æ‹¬æ€§èƒ½ç›‘æ§
"""

import os
import time
import asyncio
import tempfile
from pathlib import Path
from typing import Optional
import wave

import numpy as np
import sounddevice as sd
from openai import AsyncOpenAI
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨"""

    def __init__(self):
        self.reset()

    def reset(self):
        """é‡ç½®æ‰€æœ‰æŒ‡æ ‡"""
        self.recording_time = 0.0
        self.stt_time = 0.0
        self.llm_time = 0.0
        self.tts_time = 0.0
        self.playback_time = 0.0
        self.total_time = 0.0
        self.transcribed_text = ""
        self.llm_response = ""

    def print_summary(self):
        """æ‰“å°æ€§èƒ½æ‘˜è¦"""
        print("\n" + "="*60)
        print("æ€§èƒ½æŒ‡æ ‡æ‘˜è¦")
        print("="*60)
        print(f"å½•éŸ³æ—¶é•¿:        {self.recording_time:.2f}s")
        print(f"è¯­éŸ³è¯†åˆ«(STT):   {self.stt_time:.2f}s")
        print(f"LLMå¤„ç†:         {self.llm_time:.2f}s")
        print(f"è¯­éŸ³åˆæˆ(TTS):   {self.tts_time:.2f}s")
        print(f"éŸ³é¢‘æ’­æ”¾:        {self.playback_time:.2f}s")
        print(f"-" * 60)
        print(f"æ€»è€—æ—¶:          {self.total_time:.2f}s")
        print(f"éå½•éŸ³è€—æ—¶:      {self.total_time - self.recording_time:.2f}s")
        print("="*60)
        if self.transcribed_text:
            print(f"\nç”¨æˆ·: {self.transcribed_text}")
        if self.llm_response:
            print(f"åŠ©æ‰‹: {self.llm_response}")
        print()


class RealtimeVoiceAgent:
    """å®æ—¶è¯­éŸ³äº¤äº’Agent"""

    def __init__(
        self,
        api_key: Optional[str] = None,
        model: str = "gpt-4o-mini",
        stt_model: str = "whisper-1",
        tts_model: str = "tts-1",
        tts_voice: str = "alloy"
    ):
        """
        åˆå§‹åŒ–è¯­éŸ³äº¤äº’Agent

        Args:
            api_key: OpenAI APIå¯†é’¥
            model: LLMæ¨¡å‹åç§°
            stt_model: è¯­éŸ³è¯†åˆ«æ¨¡å‹
            tts_model: è¯­éŸ³åˆæˆæ¨¡å‹
            tts_voice: TTSè¯­éŸ³ç±»å‹ (alloy, echo, fable, onyx, nova, shimmer)
        """
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("æœªæ‰¾åˆ° OPENAI_API_KEYï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡æˆ–ä¼ å…¥å‚æ•°")

        self.client = AsyncOpenAI(api_key=self.api_key)
        self.model = model
        self.stt_model = stt_model
        self.tts_model = tts_model
        self.tts_voice = tts_voice

        # å¯¹è¯å†å²
        self.conversation_history = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ï¼Œè¯·ç”¨ç®€æ´æ˜äº†çš„æ–¹å¼å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"}
        ]

        # å½•éŸ³å‚æ•°
        self.sample_rate = 16000
        self.channels = 1

        # æ€§èƒ½æŒ‡æ ‡
        self.metrics = PerformanceMetrics()

    def _get_volume_bar(self, volume: float, bar_length: int = 30) -> str:
        """
        ç”ŸæˆéŸ³é‡å¯è§†åŒ–æ¡

        Args:
            volume: éŸ³é‡å€¼ï¼ˆ0-1ï¼‰
            bar_length: è¿›åº¦æ¡é•¿åº¦

        Returns:
            å¯è§†åŒ–å­—ç¬¦ä¸²
        """
        filled = int(volume * bar_length)
        bar = "â–ˆ" * filled + "â–‘" * (bar_length - filled)
        percentage = int(volume * 100)
        return f"|{bar}| {percentage}%"

    def _check_audio_quality(self, audio_data: np.ndarray) -> dict:
        """
        æ£€æŸ¥éŸ³é¢‘è´¨é‡

        Args:
            audio_data: éŸ³é¢‘æ•°æ®æ•°ç»„

        Returns:
            åŒ…å«è´¨é‡æŒ‡æ ‡çš„å­—å…¸
        """
        # è®¡ç®—æ•´ä½“éŸ³é‡ï¼ˆRMSï¼‰
        rms = np.sqrt(np.mean(audio_data ** 2))

        # è®¡ç®—å³°å€¼éŸ³é‡
        peak = np.max(np.abs(audio_data))

        # è®¡ç®—éé™éŸ³æ¯”ä¾‹ï¼ˆéŸ³é‡é«˜äºé˜ˆå€¼çš„éƒ¨åˆ†ï¼‰
        voice_threshold = 0.02
        voice_ratio = np.sum(np.abs(audio_data) > voice_threshold) / len(audio_data)

        # è®¡ç®—åŠ¨æ€èŒƒå›´
        dynamic_range = peak / (rms + 1e-10)

        return {
            "rms": rms,
            "peak": peak,
            "voice_ratio": voice_ratio,
            "dynamic_range": dynamic_range,
            "duration": len(audio_data) / self.sample_rate
        }

    def record_audio(
        self,
        duration: float = 5.0,
        silence_threshold: float = 0.015,
        min_voice_energy: float = 0.02,
        show_volume: bool = True
    ) -> Optional[str]:
        """
        å½•åˆ¶éŸ³é¢‘ï¼ˆå¸¦è´¨é‡æ£€æµ‹ï¼‰

        Args:
            duration: æœ€å¤§å½•éŸ³æ—¶é•¿ï¼ˆç§’ï¼‰
            silence_threshold: é™éŸ³é˜ˆå€¼
            min_voice_energy: æœ€å°è¯­éŸ³èƒ½é‡é˜ˆå€¼
            show_volume: æ˜¯å¦æ˜¾ç¤ºå®æ—¶éŸ³é‡

        Returns:
            ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœå½•éŸ³è´¨é‡ä¸åˆæ ¼åˆ™è¿”å›None
        """
        print(f"\nğŸ¤ å¼€å§‹å½•éŸ³ (æœ€é•¿ {duration} ç§’ï¼Œè¯´å®Œåä¿æŒå®‰é™å³å¯è‡ªåŠ¨åœæ­¢)...")
        if show_volume:
            print("ğŸ’¡ æç¤º: è¯·åœ¨å®‰é™ç¯å¢ƒä¸­å¯¹ç€éº¦å…‹é£æ¸…æ™°è¯´è¯\n")

        start_time = time.time()
        recording = []
        silence_duration = 0
        max_silence = 1.5  # 1.5ç§’é™éŸ³ååœæ­¢
        has_speech = False  # æ˜¯å¦æ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³
        max_volume_seen = 0.0

        def audio_callback(indata, frames, time_info, status):
            """éŸ³é¢‘å›è°ƒå‡½æ•°"""
            if status:
                print(f"å½•éŸ³çŠ¶æ€: {status}")
            recording.append(indata.copy())

        # å¼€å§‹å½•éŸ³
        with sd.InputStream(
            samplerate=self.sample_rate,
            channels=self.channels,
            callback=audio_callback,
            dtype=np.float32
        ):
            chunk_duration = 0.1  # æ¯100msæ£€æŸ¥ä¸€æ¬¡
            chunks = int(duration / chunk_duration)

            for _ in range(chunks):
                sd.sleep(int(chunk_duration * 1000))

                # æ£€æŸ¥æœ€è¿‘çš„éŸ³é¢‘å—
                if len(recording) > 0:
                    recent_chunk = recording[-1]
                    volume = np.abs(recent_chunk).mean()
                    max_volume_seen = max(max_volume_seen, volume)

                    # æ˜¾ç¤ºå®æ—¶éŸ³é‡
                    if show_volume and len(recording) % 5 == 0:  # æ¯0.5ç§’æ›´æ–°ä¸€æ¬¡
                        normalized_volume = min(volume / 0.1, 1.0)  # å½’ä¸€åŒ–åˆ°0-1
                        print(f"\réŸ³é‡: {self._get_volume_bar(normalized_volume)}", end="", flush=True)

                    # æ£€æµ‹æ˜¯å¦æœ‰æœ‰æ•ˆè¯­éŸ³
                    if volume > min_voice_energy:
                        has_speech = True

                    # æ£€æµ‹é™éŸ³
                    if volume < silence_threshold:
                        silence_duration += chunk_duration
                        # åªæœ‰åœ¨æ£€æµ‹åˆ°è¯­éŸ³åæ‰èƒ½é€šè¿‡é™éŸ³åœæ­¢
                        if silence_duration >= max_silence and has_speech and len(recording) > 10:
                            print("\næ£€æµ‹åˆ°é™éŸ³ï¼Œåœæ­¢å½•éŸ³")
                            break
                    else:
                        silence_duration = 0

        if show_volume:
            print()  # æ¢è¡Œ

        self.metrics.recording_time = time.time() - start_time

        # åˆå¹¶å½•éŸ³æ•°æ®
        if not recording:
            print("âŒ å½•éŸ³å¤±è´¥ï¼šæ²¡æœ‰å½•åˆ¶åˆ°ä»»ä½•æ•°æ®")
            return None

        audio_data = np.concatenate(recording, axis=0)

        # æ£€æŸ¥éŸ³é¢‘è´¨é‡
        quality = self._check_audio_quality(audio_data)

        print(f"âœ“ å½•éŸ³å®Œæˆ ({self.metrics.recording_time:.2f}s)")
        print(f"ğŸ“Š éŸ³é¢‘è´¨é‡æ£€æµ‹:")
        print(f"   - å¹³å‡éŸ³é‡(RMS): {quality['rms']:.4f}")
        print(f"   - å³°å€¼éŸ³é‡: {quality['peak']:.4f}")
        print(f"   - è¯­éŸ³å æ¯”: {quality['voice_ratio']*100:.1f}%")

        # è´¨é‡éªŒè¯
        if quality['rms'] < 0.005:
            print("âŒ éŸ³é¢‘è´¨é‡ä¸åˆæ ¼ï¼šå½•éŸ³éŸ³é‡å¤ªä½ï¼ˆå¯èƒ½æ²¡æœ‰è¯´è¯æˆ–éº¦å…‹é£æœªå·¥ä½œï¼‰")
            print("   å»ºè®®ï¼š")
            print("   1. æ£€æŸ¥éº¦å…‹é£æ˜¯å¦æ­£å¸¸å·¥ä½œ")
            print("   2. ç¡®ä¿åœ¨å®‰é™ç¯å¢ƒä¸­è¯´è¯")
            print("   3. ç¦»éº¦å…‹é£è¿‘ä¸€äº›ï¼Œè¯´è¯å£°éŸ³æ¸…æ™°ä¸€äº›")
            return None

        if quality['voice_ratio'] < 0.1:
            print("âš ï¸  è­¦å‘Šï¼šå½•éŸ³ä¸­æœ‰æ•ˆè¯­éŸ³å†…å®¹è¾ƒå°‘ï¼ˆå¯èƒ½å½•åˆ¶äº†èƒŒæ™¯éŸ³ï¼‰")
            print("   å»ºè®®ï¼šå…³é—­å…¶ä»–æ­£åœ¨æ’­æ”¾çš„éŸ³é¢‘/è§†é¢‘")
            return None

        if not has_speech:
            print("âŒ æœªæ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³ä¿¡å·")
            return None

        print("âœ“ éŸ³é¢‘è´¨é‡åˆæ ¼")

        # ä¿å­˜ä¸ºä¸´æ—¶WAVæ–‡ä»¶
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
        with wave.open(temp_file.name, 'wb') as wf:
            wf.setnchannels(self.channels)
            wf.setsampwidth(2)  # 16-bit
            wf.setframerate(self.sample_rate)
            # è½¬æ¢ä¸º16-bit PCM
            audio_int16 = (audio_data * 32767).astype(np.int16)
            wf.writeframes(audio_int16.tobytes())

        return temp_file.name

    async def speech_to_text(self, audio_file_path: str) -> str:
        """
        è¯­éŸ³è½¬æ–‡æœ¬

        Args:
            audio_file_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„

        Returns:
            è¯†åˆ«å‡ºçš„æ–‡æœ¬
        """
        print("ğŸ”„ æ­£åœ¨è¿›è¡Œè¯­éŸ³è¯†åˆ«...")
        start_time = time.time()

        with open(audio_file_path, "rb") as audio_file:
            transcript = await self.client.audio.transcriptions.create(
                model=self.stt_model,
                file=audio_file,
                language="zh"  # æŒ‡å®šä¸­æ–‡å¯ä»¥æé«˜è¯†åˆ«å‡†ç¡®åº¦
            )

        self.metrics.stt_time = time.time() - start_time
        text = transcript.text
        self.metrics.transcribed_text = text

        print(f"âœ“ è¯†åˆ«å®Œæˆ ({self.metrics.stt_time:.2f}s): {text}")
        return text

    async def get_llm_response(self, user_input: str) -> str:
        """
        è·å–LLMå“åº”

        Args:
            user_input: ç”¨æˆ·è¾“å…¥æ–‡æœ¬

        Returns:
            LLMå“åº”æ–‡æœ¬
        """
        print("ğŸ¤– æ­£åœ¨ç”Ÿæˆå›å¤...")
        start_time = time.time()

        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        self.conversation_history.append({"role": "user", "content": user_input})

        # è°ƒç”¨LLM
        response = await self.client.chat.completions.create(
            model=self.model,
            messages=self.conversation_history,
            temperature=0.7,
            max_tokens=500
        )

        assistant_message = response.choices[0].message.content
        self.conversation_history.append({"role": "assistant", "content": assistant_message})

        self.metrics.llm_time = time.time() - start_time
        self.metrics.llm_response = assistant_message

        print(f"âœ“ å›å¤ç”Ÿæˆå®Œæˆ ({self.metrics.llm_time:.2f}s)")
        return assistant_message

    async def text_to_speech(self, text: str) -> str:
        """
        æ–‡æœ¬è½¬è¯­éŸ³

        Args:
            text: è¦è½¬æ¢çš„æ–‡æœ¬

        Returns:
            éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        """
        print("ğŸ”Š æ­£åœ¨åˆæˆè¯­éŸ³...")
        start_time = time.time()

        response = await self.client.audio.speech.create(
            model=self.tts_model,
            voice=self.tts_voice,
            input=text,
            response_format="mp3"
        )

        # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
        temp_file.write(response.content)
        temp_file.close()

        self.metrics.tts_time = time.time() - start_time
        print(f"âœ“ è¯­éŸ³åˆæˆå®Œæˆ ({self.metrics.tts_time:.2f}s)")

        return temp_file.name

    def play_audio(self, audio_file_path: str):
        """
        æ’­æ”¾éŸ³é¢‘æ–‡ä»¶

        Args:
            audio_file_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        """
        print("â–¶ï¸  æ­£åœ¨æ’­æ”¾å›å¤...")
        start_time = time.time()

        # ä½¿ç”¨ç³»ç»Ÿé»˜è®¤æ’­æ”¾å™¨ï¼ˆmacOSï¼‰
        if os.system(f'afplay "{audio_file_path}"') != 0:
            print("âš ï¸  éŸ³é¢‘æ’­æ”¾å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç³»ç»ŸéŸ³é¢‘è®¾ç½®")

        self.metrics.playback_time = time.time() - start_time
        print(f"âœ“ æ’­æ”¾å®Œæˆ ({self.metrics.playback_time:.2f}s)")

    async def process_voice_input(self, recording_duration: float = 5.0):
        """
        å¤„ç†ä¸€è½®è¯­éŸ³äº¤äº’

        Args:
            recording_duration: å½•éŸ³æ—¶é•¿ï¼ˆç§’ï¼‰
        """
        self.metrics.reset()
        total_start = time.time()

        audio_file = None
        response_audio = None

        try:
            # 1. å½•éŸ³
            audio_file = self.record_audio(duration=recording_duration)

            # å¦‚æœå½•éŸ³è´¨é‡ä¸åˆæ ¼ï¼Œè¿”å›
            if audio_file is None:
                print("\nâš ï¸  å½•éŸ³æœªé€šè¿‡è´¨é‡æ£€æµ‹ï¼Œè¯·é‡è¯•")
                print("ğŸ’¡ æç¤ºï¼šç¡®ä¿åœ¨å®‰é™ç¯å¢ƒä¸­æ¸…æ™°è¯´è¯ï¼Œå¹¶å…³é—­å…¶ä»–éŸ³é¢‘/è§†é¢‘æ’­æ”¾")
                return

            # 2. è¯­éŸ³è¯†åˆ«
            user_text = await self.speech_to_text(audio_file)

            if not user_text.strip():
                print("âš ï¸  æœªè¯†åˆ«åˆ°æœ‰æ•ˆè¯­éŸ³ï¼Œè¯·é‡è¯•")
                return

            # 3. LLMå¤„ç†
            assistant_text = await self.get_llm_response(user_text)

            # 4. è¯­éŸ³åˆæˆ
            response_audio = await self.text_to_speech(assistant_text)

            # 5. æ’­æ”¾å›å¤
            self.play_audio(response_audio)

            # è®¡ç®—æ€»è€—æ—¶
            self.metrics.total_time = time.time() - total_start

            # æ‰“å°æ€§èƒ½æŒ‡æ ‡
            self.metrics.print_summary()

        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if audio_file and os.path.exists(audio_file):
                os.unlink(audio_file)
            if response_audio and os.path.exists(response_audio):
                os.unlink(response_audio)

    async def run_interactive_loop(self):
        """è¿è¡Œäº¤äº’å¼å¾ªç¯"""
        print("\n" + "="*60)
        print("å®æ—¶è¯­éŸ³äº¤äº’Agentå·²å¯åŠ¨")
        print("="*60)
        print("ä½¿ç”¨è¯´æ˜:")
        print("- æŒ‰å›è½¦é”®å¼€å§‹å½•éŸ³")
        print("- å¯¹ç€éº¦å…‹é£æ¸…æ™°è¯´è¯ï¼ˆä¼šæ˜¾ç¤ºå®æ—¶éŸ³é‡ï¼‰")
        print("- è¯´å®Œåä¿æŒå®‰é™1.5ç§’è‡ªåŠ¨åœæ­¢")
        print("- ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹éŸ³é¢‘è´¨é‡")
        print("- è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºç¨‹åº")
        print("\né‡è¦æç¤º:")
        print("âš ï¸  è¯·åœ¨å®‰é™ç¯å¢ƒä¸­ä½¿ç”¨ï¼Œå…³é—­å…¶ä»–éŸ³é¢‘/è§†é¢‘æ’­æ”¾")
        print("âš ï¸  å¦‚æœæ²¡æœ‰è¯´è¯ï¼Œç³»ç»Ÿä¼šæ‹’ç»å¤„ç†ä½è´¨é‡å½•éŸ³")
        print("="*60 + "\n")

        while True:
            user_input = input("æŒ‰å›è½¦å¼€å§‹å¯¹è¯ (æˆ–è¾“å…¥ 'quit' é€€å‡º): ").strip().lower()

            if user_input in ['quit', 'exit', 'q']:
                print("\nğŸ‘‹ å†è§ï¼")
                break

            await self.process_voice_input()


async def main():
    """ä¸»å‡½æ•°"""
    try:
        # åˆ›å»ºAgentå®ä¾‹
        agent = RealtimeVoiceAgent(
            model="gpt-4o-mini",  # ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹
            tts_voice="nova"  # å¯é€‰: alloy, echo, fable, onyx, nova, shimmer
        )

        # è¿è¡Œäº¤äº’å¾ªç¯
        await agent.run_interactive_loop()

    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç¨‹åºå·²ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
