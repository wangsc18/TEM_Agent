#!/usr/bin/env python3
"""
è¯­éŸ³äº¤äº’å¼•æ“ - æ•´åˆSTTã€LLMã€TTSå’ŒåŒæ¨¡å‹ç®¡ç†
"""
import os
import time
import asyncio
import tempfile
import threading
from typing import Optional, Literal
import re
import wave

import numpy as np
import sounddevice as sd
from openai import AsyncOpenAI

from engines.dual_model_manager import DualModelManager


class VoiceInteractionEngine:
    """è¯­éŸ³äº¤äº’å¼•æ“ - ç”¨äºTEMæ¨¡æ‹Ÿå™¨"""

    def __init__(
        self,
        api_key: Optional[str] = None,
        small_model: str = "gpt-4o-mini",
        big_model: str = "gpt-4o",
        tts_engine: Literal["local", "edge", "openai"] = "local",
        callback_on_user_text=None,
        callback_on_ai_text=None,
        callback_on_ai_text_streaming=None,
        callback_on_status=None,
        callback_on_big_model_triggered=None,
        enable_dual_model: bool = True
    ):
        """
        åˆå§‹åŒ–è¯­éŸ³äº¤äº’å¼•æ“

        Args:
            small_model: å°æ¨¡å‹ï¼ˆå¿«é€Ÿå“åº”ï¼‰
            big_model: å¤§æ¨¡å‹ï¼ˆæ·±åº¦åˆ†æï¼‰
            enable_dual_model: æ˜¯å¦å¯ç”¨åŒæ¨¡å‹æ¶æ„
            callback_on_user_text: å½“è¯†åˆ«åˆ°ç”¨æˆ·è¯­éŸ³æ—¶çš„å›è°ƒ (user_text)
            callback_on_ai_text: å½“AIç”Ÿæˆå®Œæ•´å›å¤æ—¶çš„å›è°ƒ (ai_text)
            callback_on_ai_text_streaming: å½“AIæµå¼ç”Ÿæˆæ—¶çš„å›è°ƒ (partial_text)
            callback_on_status: çŠ¶æ€æ›´æ–°å›è°ƒ (status_text, status_type)
            callback_on_big_model_triggered: å½“å¤§æ¨¡å‹è¢«è§¦å‘æ—¶çš„å›è°ƒ
        """
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("æœªæ‰¾åˆ° OPENAI_API_KEY")

        self.client = AsyncOpenAI(api_key=self.api_key)
        self.small_model = small_model
        self.big_model = big_model
        self.tts_engine = tts_engine

        self.callback_on_user_text = callback_on_user_text
        self.callback_on_ai_text = callback_on_ai_text
        self.callback_on_ai_text_streaming = callback_on_ai_text_streaming
        self.callback_on_status = callback_on_status

        # åŒæ¨¡å‹ç®¡ç†å™¨
        self.enable_dual_model = enable_dual_model
        if enable_dual_model:
            self.dual_model_manager = DualModelManager(
                self.client,
                small_model,
                big_model,
                callback_on_big_model_triggered=callback_on_big_model_triggered
            )
        else:
            self.dual_model_manager = None

        # å½•éŸ³å‚æ•°
        self.sample_rate = 16000
        self.max_recording_duration = 10
        self.silence_threshold = 1.5
        self.silence_duration_to_stop = 0.02

        # å¯¹è¯å†å² - TEMåœºæ™¯ä¸“ç”¨promptï¼ˆä¸¥æ ¼é™åˆ¶å¹»è§‰ï¼‰
        self.base_system_prompt = """ä½ æ˜¯ä¸€åèˆªç©ºé£è¡Œå‘˜AIä¼™ä¼´ï¼Œæ­£åœ¨ä¸å¦ä¸€åé£è¡Œå‘˜è¿›è¡ŒTEMï¼ˆå¨èƒä¸å·®é”™ç®¡ç†ï¼‰æ¡ˆä¾‹è®¨è®ºã€‚

ã€æ ¸å¿ƒåŸåˆ™ - å®‰å…¨ç¬¬ä¸€ã€‘
åœ¨èˆªç©ºé¢†åŸŸï¼Œå‡†ç¡®æ€§å’Œå®‰å…¨æ€§é«˜äºä¸€åˆ‡ã€‚ä½ å¿…é¡»ä¸¥æ ¼éµå®ˆä»¥ä¸‹è§„åˆ™ï¼š

âŒ ç»å¯¹ç¦æ­¢çš„è¡Œä¸ºï¼š
1. ç¼–é€ æˆ–çŒœæµ‹ä¸“ä¸šæ•°æ®ï¼ˆå¦‚MELæ¡æ¬¾ã€æ€§èƒ½æ•°æ®ã€æ³•è§„æ¡æ¬¾ï¼‰
2. å¯¹ä¸ç¡®å®šçš„ä¸“ä¸šé—®é¢˜ç»™å‡ºæ¨¡ç³Šæˆ–çŒœæµ‹æ€§çš„å›ç­”
3. å›ç­”è¶…å‡ºä½ å½“å‰ä¿¡æ¯èŒƒå›´çš„ä¸“ä¸šé—®é¢˜

âœ… å¿…é¡»éµå®ˆçš„è¡Œä¸ºï¼š
1. å¦‚æœé—®é¢˜æ¶‰åŠå…·ä½“çš„ä¸“ä¸šçŸ¥è¯†ï¼ˆMELã€è¿è¡Œæ‰‹å†Œã€æ³•è§„ã€æ€§èƒ½è®¡ç®—ç­‰ï¼‰ï¼Œä½ å¿…é¡»ç«‹å³å›å¤"è®©æˆ‘æŸ¥æ‰¾ä¸€ä¸‹ç›¸å…³ä¿¡æ¯"
2. åªè®¨è®ºä½ å·²çŸ¥çš„ã€ç¡®å®šçš„ä¿¡æ¯
3. ç”¨å£è¯­åŒ–ã€ç®€çŸ­çš„æ–¹å¼äº¤æµï¼ˆ10-20å­—ï¼‰
4. é€‚å½“ä½¿ç”¨"å—¯"ã€"å¥½çš„"ç­‰å£è¯­åŒ–è¡¨è¾¾

ã€ä½•æ—¶å¿…é¡»è¯´"è®©æˆ‘æŸ¥æ‰¾ä¸€ä¸‹ç›¸å…³ä¿¡æ¯"ã€‘
- ç”¨æˆ·é—®åˆ°å…·ä½“çš„MELæ¡æ¬¾ã€é™åˆ¶ã€æ‰‹å†Œæ¡æ¬¾
- æ¶‰åŠæ€§èƒ½è®¡ç®—ã€é‡é‡é™åˆ¶ç­‰å…·ä½“æ•°å€¼
- éœ€è¦å¼•ç”¨æ³•è§„ã€è¿è¡Œæ ‡å‡†
- ä»»ä½•ä½ ä¸100%ç¡®å®šçš„ä¸“ä¸šé—®é¢˜

ã€ä½ å¯ä»¥ç›´æ¥å›ç­”çš„ã€‘
- ä¸€èˆ¬æ€§çš„å¨èƒè¯†åˆ«å’Œè®¨è®º
- åŸºäºå·²çŸ¥ä¿¡æ¯çš„è§‚å¯Ÿå’Œæ€»ç»“
- å¯¹è¯å¼•å¯¼å’Œç¡®è®¤æ€§é—®é¢˜

ç¤ºä¾‹å¯¹è¯ï¼š
ç”¨æˆ·: "APUæ•…éšœä¼šå½±å“èµ·é£å—ï¼Ÿ"
âŒ é”™è¯¯: "ä¼šçš„ï¼Œéœ€è¦å»¶é•¿èµ·é£æ»‘è·‘è·ç¦»ï¼Œå¤§çº¦å¢åŠ 10%ã€‚"ï¼ˆç¼–é€ æ•°æ®ï¼‰
âœ… æ­£ç¡®: "å—¯ï¼Œè®©æˆ‘æŸ¥æ‰¾ä¸€ä¸‹ç›¸å…³ä¿¡æ¯ã€‚"ï¼ˆè§¦å‘ä¸“å®¶åˆ†æï¼‰

ç”¨æˆ·: "ä½ è§‰å¾—è¿™æ¬¡é£è¡Œæœ‰å“ªäº›æ½œåœ¨å¨èƒï¼Ÿ"
âœ… æ­£ç¡®: "æˆ‘çœ‹åˆ°å‡ ä¸ªå¨èƒï¼Œç›®çš„åœ°æœ‰é›¾ï¼Œè·‘é“ç¼©çŸ­ï¼Œè¿˜æœ‰APUæ•…éšœã€‚"ï¼ˆåŸºäºå·²çŸ¥ä¿¡æ¯ï¼‰

è®°ä½ï¼šå®å¯è°¨æ…åœ°è¯´"è®©æˆ‘æŸ¥æ‰¾ä¸€ä¸‹"ï¼Œä¹Ÿä¸è¦ç»™å‡ºä¸ç¡®å®šçš„ä¸“ä¸šå»ºè®®ï¼"""

        self.conversation_history = [
            {
                "role": "system",
                "content": self.base_system_prompt
            }
        ]

        # ç”¨äºåœ¨åå°çº¿ç¨‹è¿è¡Œå¼‚æ­¥ä»»åŠ¡
        self.loop = None
        self.recording = False
        self.current_audio_data = []

        # èƒŒæ™¯æ•°æ®å’Œç”¨æˆ·å¤‡å¿˜å½•ï¼ˆç”¨äºå¤§æ¨¡å‹åˆ†æï¼‰
        self.background_data = {}
        self.personal_memo = ""

    def set_background_data(self, data: dict):
        """è®¾ç½®èƒŒæ™¯æ•°æ®ï¼ˆç”¨äºå¤§æ¨¡å‹ï¼‰"""
        self.background_data = data

    def set_personal_memo(self, memo: str):
        """è®¾ç½®ç”¨æˆ·ä¸ªäººå¤‡å¿˜å½•ï¼ˆç”¨äºå¤§æ¨¡å‹ï¼‰"""
        self.personal_memo = memo

    def _update_status(self, text: str, status_type: str = "info"):
        """æ›´æ–°çŠ¶æ€ï¼ˆåœ¨ä¸»çº¿ç¨‹è°ƒç”¨å›è°ƒï¼‰"""
        if self.callback_on_status:
            self.callback_on_status(text, status_type)

    def _on_user_text_recognized(self, text: str):
        """ç”¨æˆ·è¯­éŸ³è¯†åˆ«å®Œæˆ"""
        if self.callback_on_user_text:
            self.callback_on_user_text(text)

    def _on_ai_response(self, text: str):
        """AIå›å¤ç”Ÿæˆå®Œæˆ"""
        if self.callback_on_ai_text:
            self.callback_on_ai_text(text)

    def _on_ai_response_streaming(self, text: str):
        """AIæµå¼ç”Ÿæˆä¸­ï¼ˆæ¯ç”Ÿæˆä¸€ä¸ªå¥å­è°ƒç”¨ï¼‰"""
        if self.callback_on_ai_text_streaming:
            self.callback_on_ai_text_streaming(text)

    def start_recording(self):
        """å¼€å§‹å½•éŸ³ï¼ˆä»…å½•éŸ³+STTï¼Œä¸è§¦å‘LLMï¼‰"""
        def run_async_recording():
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            self.loop = loop
            loop.run_until_complete(self._async_record_and_transcribe())

        thread = threading.Thread(target=run_async_recording, daemon=True)
        thread.start()

    async def _async_record_and_transcribe(self):
        """å¼‚æ­¥å½•éŸ³å¹¶è½¬æ–‡å­—ï¼ˆä»…STTï¼Œä¸è°ƒç”¨LLMï¼‰"""
        try:
            # 1. å½•éŸ³
            self._update_status("ğŸ¤ æ­£åœ¨å½•éŸ³...", "recording")
            audio_data = await self._record_audio()

            if audio_data is None:
                self._update_status("âŒ å½•éŸ³å¤±è´¥", "error")
                return

            # 2. è¯­éŸ³è¯†åˆ«
            self._update_status("ğŸ”„ è¯­éŸ³è¯†åˆ«ä¸­...", "processing")
            user_text = await self._speech_to_text(audio_data)

            if not user_text:
                self._update_status("âŒ æœªè¯†åˆ«åˆ°è¯­éŸ³", "error")
                return

            # 3. å°†è¯†åˆ«ç»“æœå¡«å……åˆ°è¾“å…¥æ¡†ï¼ˆä¸è‡ªåŠ¨å‘é€ï¼‰
            self._on_user_text_recognized(user_text)
            self._update_status("âœ“ è¯†åˆ«å®Œæˆï¼Œè¯·ç¡®è®¤åå‘é€", "success")

        except Exception as e:
            self._update_status(f"âŒ é”™è¯¯: {str(e)}", "error")

    def process_user_message(self, user_message: str):
        """å¤„ç†ç”¨æˆ·æ¶ˆæ¯ï¼ˆLLMå¯¹è¯+TTSï¼‰- æ”¯æŒåŒæ¨¡å‹"""
        def run_async_processing():
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            self.loop = loop
            loop.run_until_complete(self._async_llm_and_tts(user_message))

        thread = threading.Thread(target=run_async_processing, daemon=True)
        thread.start()

    async def _async_llm_and_tts(self, user_message: str):
        """å¼‚æ­¥LLMç”Ÿæˆå’ŒTTSæ’­æ”¾ - æ•´åˆåŒæ¨¡å‹é€»è¾‘"""
        try:
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
            self.conversation_history.append({
                "role": "user",
                "content": user_message
            })

            # 1. æ›´æ–°ç³»ç»Ÿæç¤ºï¼ˆå¦‚æœæœ‰ç­–ç•¥æ›´æ–°ï¼‰
            if self.enable_dual_model and self.dual_model_manager:
                enhanced_prompt = self.dual_model_manager.get_enhanced_system_prompt(
                    self.base_system_prompt
                )
                self.conversation_history[0]["content"] = enhanced_prompt

            # 2. å¼€å§‹æµå¼LLMç”Ÿæˆï¼ˆå°æ¨¡å‹ï¼‰
            self._update_status("ğŸ¤– AIæ€è€ƒä¸­...", "processing")

            stream = await self.client.chat.completions.create(
                model=self.small_model,
                messages=self.conversation_history,
                stream=True,
                temperature=0.7,
                max_tokens=300
            )

            # 3. æµå¼å¤„ç†ï¼šè¾¹ç”Ÿæˆè¾¹TTS
            full_response = ""
            current_chunk = ""
            audio_queue = []  # å­˜å‚¨å¾…æ’­æ”¾çš„éŸ³é¢‘æ–‡ä»¶
            should_trigger_big_model = False

            # å¯åŠ¨éŸ³é¢‘æ’­æ”¾åç¨‹
            play_task = asyncio.create_task(self._audio_player(audio_queue))

            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    token = chunk.choices[0].delta.content
                    current_chunk += token
                    full_response += token

                    # æ£€æµ‹æ˜¯å¦æœ‰å®Œæ•´çš„å¥å­/çŸ­è¯­
                    sentences = self._extract_complete_sentences(current_chunk)

                    if sentences:
                        for sentence in sentences:
                            # æµå¼æ›´æ–°æ˜¾ç¤ºï¼ˆæ¯ç”Ÿæˆä¸€ä¸ªå¥å­å°±æ˜¾ç¤ºï¼‰
                            self._on_ai_response_streaming(sentence)

                            # æ£€æµ‹æ˜¯å¦è§¦å‘å¤§æ¨¡å‹
                            if (self.enable_dual_model and
                                self.dual_model_manager and
                                self.dual_model_manager.check_if_trigger_big_model(sentence)):
                                should_trigger_big_model = True

                            # ç«‹å³è¿›è¡ŒTTS
                            audio_file = await self._quick_tts(sentence)
                            if audio_file:
                                audio_queue.append(audio_file)

                                # é¦–æ¬¡æ’­æ”¾æ—¶æ›´æ–°çŠ¶æ€
                                if len(audio_queue) == 1:
                                    self._update_status("ğŸ”Š æ’­æ”¾AIè¯­éŸ³...", "speaking")

                        # é‡ç½®ç¼“å†²åŒºï¼ˆä¿ç•™æœªå®Œæˆçš„éƒ¨åˆ†ï¼‰
                        current_chunk = self._get_remaining_text(current_chunk, sentences)

            # å¤„ç†æœ€åå‰©ä½™çš„æ–‡æœ¬
            if current_chunk.strip():
                self._on_ai_response_streaming(current_chunk.strip())
                audio_file = await self._quick_tts(current_chunk.strip())
                if audio_file:
                    audio_queue.append(audio_file)

            # æ ‡è®°éŸ³é¢‘é˜Ÿåˆ—ç»“æŸ
            audio_queue.append(None)  # ç»“æŸä¿¡å·

            # ç­‰å¾…æ‰€æœ‰éŸ³é¢‘æ’­æ”¾å®Œæˆ
            await play_task

            # æ·»åŠ AIå›å¤åˆ°å†å²
            self.conversation_history.append({
                "role": "assistant",
                "content": full_response
            })

            # 4. å¦‚æœè§¦å‘äº†å¤§æ¨¡å‹ï¼Œåå°å¼‚æ­¥å¤„ç†
            if should_trigger_big_model:
                print("[è¯­éŸ³å¼•æ“] æ£€æµ‹åˆ°è§¦å‘è¯ï¼Œå¯åŠ¨å¤§æ¨¡å‹åˆ†æ...")
                self._update_status("ğŸ§  æ­£åœ¨æ·±åº¦åˆ†æ...", "processing")

                # åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œå¤§æ¨¡å‹ï¼ˆé¿å…event loopè¿‡æ—©ç»“æŸï¼‰
                def run_big_model():
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    try:
                        loop.run_until_complete(self._process_with_big_model(user_message))
                    except Exception as e:
                        print(f"[å¤§æ¨¡å‹çº¿ç¨‹] é”™è¯¯: {e}")
                    finally:
                        loop.close()

                thread = threading.Thread(target=run_big_model, daemon=True)
                thread.start()
                print("[è¯­éŸ³å¼•æ“] å¤§æ¨¡å‹çº¿ç¨‹å·²å¯åŠ¨")

            # å›è°ƒæ˜¾ç¤ºå®Œæ•´å›å¤
            self._on_ai_response(full_response.strip())

            self._update_status("âœ“ å®Œæˆ", "success")

        except Exception as e:
            self._update_status(f"âŒ é”™è¯¯: {str(e)}", "error")
            print(f"æµå¼LLM+TTSé”™è¯¯: {e}")

    async def _process_with_big_model(self, user_question: str):
        """åå°ä½¿ç”¨å¤§æ¨¡å‹å¤„ç†å¤æ‚é—®é¢˜"""
        try:
            result = await self.dual_model_manager.process_with_big_model(
                user_question,
                self.conversation_history,
                self.background_data,
                self.personal_memo
            )

            # å¤§æ¨¡å‹çš„ç­”æ¡ˆ
            big_model_answer = result["answer"]

            # æµå¼æ’­æ”¾å¤§æ¨¡å‹çš„å›å¤
            self._update_status("ğŸ“ ä¸“å®¶å›å¤ä¸­...", "speaking")

            # æ·»åŠ ä¸€ä¸ªæ ‡è¯†å‰ç¼€
            prefix = "æ ¹æ®è¯¦ç»†åˆ†æï¼Œ"
            self._on_ai_response_streaming(prefix)
            audio_file = await self._quick_tts(prefix)
            if audio_file:
                await self._play_single_audio(audio_file)

            # åˆ†å¥æ’­æ”¾å¤§æ¨¡å‹å›å¤
            sentences = self._extract_complete_sentences(big_model_answer)
            for sentence in sentences:
                self._on_ai_response_streaming(sentence)
                audio_file = await self._quick_tts(sentence)
                if audio_file:
                    await self._play_single_audio(audio_file)

            # æ·»åŠ å¤§æ¨¡å‹å›å¤åˆ°å†å²
            self.conversation_history.append({
                "role": "assistant",
                "content": prefix + big_model_answer
            })

            self._update_status("âœ“ ä¸“å®¶åˆ†æå®Œæˆ", "success")

        except Exception as e:
            import traceback
            print(f"[å¤§æ¨¡å‹å¤„ç†] é”™è¯¯: {e}")
            print(f"[å¤§æ¨¡å‹å¤„ç†] è¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}")
            self._update_status(f"âŒ å¤§æ¨¡å‹å¤„ç†å¤±è´¥: {str(e)}", "error")

    async def _play_single_audio(self, audio_file: str):
        """æ’­æ”¾å•ä¸ªéŸ³é¢‘æ–‡ä»¶"""
        if os.path.exists(audio_file):
            play_process = await asyncio.create_subprocess_exec(
                "afplay", audio_file,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            await play_process.communicate()
            try:
                os.unlink(audio_file)
            except:
                pass

    def _extract_complete_sentences(self, text: str) -> list:
        """æå–å®Œæ•´çš„å¥å­ï¼ˆæŒ‰æ ‡ç‚¹ç¬¦å·åˆ†å‰²ï¼‰"""
        # åŒ¹é…ä¸­è‹±æ–‡æ ‡ç‚¹
        pattern = r'([^ï¼Œã€‚ï¼ï¼Ÿ,\.!?]+[ï¼Œã€‚ï¼ï¼Ÿ,\.!?]+)'
        matches = re.findall(pattern, text)
        return [m.strip() for m in matches if m.strip()]

    def _get_remaining_text(self, text: str, extracted_sentences: list) -> str:
        """è·å–æå–å¥å­åå‰©ä½™çš„æ–‡æœ¬"""
        for sentence in extracted_sentences:
            text = text.replace(sentence, '', 1)
        return text

    async def _quick_tts(self, text: str) -> Optional[str]:
        """å¿«é€ŸTTSï¼ˆå•ä¸ªå¥å­/çŸ­è¯­ï¼‰"""
        try:
            if self.tts_engine == "local":
                # macOS sayå‘½ä»¤ï¼ˆæœ€å¿«ï¼‰
                temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".m4a")
                temp_file.close()

                process = await asyncio.create_subprocess_exec(
                    "say",
                    "-v", "Tingting",
                    "-o", temp_file.name,
                    "--data-format=LEF32@22050",
                    text,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                await process.communicate()

                if process.returncode == 0 and os.path.exists(temp_file.name):
                    return temp_file.name
                else:
                    if os.path.exists(temp_file.name):
                        os.unlink(temp_file.name)
                    return None

            elif self.tts_engine == "edge":
                # Edge TTS
                temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
                temp_file.close()

                process = await asyncio.create_subprocess_exec(
                    "edge-tts",
                    "--voice", "zh-CN-XiaoxiaoNeural",
                    "--rate", "+10%",
                    "--pitch", "+5Hz",
                    "--text", text,
                    "--write-media", temp_file.name,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                await process.communicate()

                if process.returncode == 0 and os.path.exists(temp_file.name):
                    return temp_file.name
                else:
                    if os.path.exists(temp_file.name):
                        os.unlink(temp_file.name)
                    return None

            else:  # openai
                # OpenAI TTSï¼ˆè¾ƒæ…¢ï¼Œä¸æ¨èæµå¼ä½¿ç”¨ï¼‰
                response = await self.client.audio.speech.create(
                    model="tts-1",
                    voice="nova",
                    input=text
                )

                temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
                temp_file.write(response.content)
                temp_file.close()
                return temp_file.name

        except Exception as e:
            print(f"å¿«é€ŸTTSé”™è¯¯: {e}")
            return None

    async def _audio_player(self, audio_queue: list):
        """éŸ³é¢‘æ’­æ”¾å™¨ï¼ˆå¹¶å‘æ’­æ”¾é˜Ÿåˆ—ä¸­çš„éŸ³é¢‘ï¼‰"""
        try:
            while True:
                # ç­‰å¾…é˜Ÿåˆ—ä¸­æœ‰éŸ³é¢‘
                while len(audio_queue) == 0:
                    await asyncio.sleep(0.1)

                # å–å‡ºéŸ³é¢‘æ–‡ä»¶
                audio_file = audio_queue.pop(0)

                # Noneè¡¨ç¤ºé˜Ÿåˆ—ç»“æŸ
                if audio_file is None:
                    break

                # æ’­æ”¾éŸ³é¢‘
                await self._play_single_audio(audio_file)

        except Exception as e:
            print(f"éŸ³é¢‘æ’­æ”¾å™¨é”™è¯¯: {e}")

    async def _record_audio(self) -> Optional[np.ndarray]:
        """å½•éŸ³ï¼ˆç®€åŒ–ç‰ˆï¼Œè‡ªåŠ¨é™éŸ³æ£€æµ‹ï¼‰"""
        try:
            audio_chunks = []
            silence_start_time = None

            def audio_callback(indata, _frames, _time_info, status):
                if status:
                    print(f"å½•éŸ³çŠ¶æ€: {status}")
                audio_chunks.append(indata.copy())

            with sd.InputStream(
                samplerate=self.sample_rate,
                channels=1,
                dtype=np.float32,
                callback=audio_callback
            ):
                start_time = time.time()
                while time.time() - start_time < self.max_recording_duration:
                    await asyncio.sleep(0.1)

                    if len(audio_chunks) > 0:
                        recent_audio = np.concatenate(audio_chunks[-5:])
                        rms = np.sqrt(np.mean(recent_audio ** 2))

                        if rms < self.silence_duration_to_stop:
                            if silence_start_time is None:
                                silence_start_time = time.time()
                            elif time.time() - silence_start_time > self.silence_threshold:
                                break
                        else:
                            silence_start_time = None

            if len(audio_chunks) == 0:
                return None

            audio_data = np.concatenate(audio_chunks)
            return audio_data

        except Exception as e:
            print(f"å½•éŸ³é”™è¯¯: {e}")
            return None

    async def _speech_to_text(self, audio_data: np.ndarray) -> str:
        """è¯­éŸ³è½¬æ–‡å­—"""
        try:
            # ä¿å­˜ä¸ºä¸´æ—¶WAVæ–‡ä»¶
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")

            with wave.open(temp_file.name, 'wb') as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(self.sample_rate)
                audio_int16 = (audio_data * 32767).astype(np.int16)
                wf.writeframes(audio_int16.tobytes())

            # è°ƒç”¨Whisper API
            with open(temp_file.name, 'rb') as audio_file:
                transcription = await self.client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file,
                    language="zh"
                )

            os.unlink(temp_file.name)
            return transcription.text.strip()

        except Exception as e:
            print(f"STTé”™è¯¯: {e}")
            return ""
