#!/usr/bin/env python3
"""
ä¼ ç»Ÿæ–‡æœ¬ LLM å¼•æ“ - å¤§æ¨¡å‹ä¸“ç”¨
ä½¿ç”¨è‡ªå®šä¹‰ API ç«¯ç‚¹ï¼ˆå¦‚ yunwu å¹³å°ï¼‰ï¼Œè¿”å›æ–‡æœ¬å“åº”
"""
import os
from typing import Optional, List, Dict
from openai import AsyncOpenAI


class TextLLMEngine:
    """ä¼ ç»Ÿæ–‡æœ¬ LLM å¼•æ“ï¼ˆå¤§æ¨¡å‹ï¼‰"""

    def __init__(
        self,
        api_key: str,
        base_url: str,
        model: str = "gpt-4o",
        system_prompt: str = "",
        temperature: float = 0.7,
        max_tokens: int = 2000,
        callback_on_response_start=None,
        callback_on_text_delta=None,
        callback_on_response_done=None,
        callback_on_error=None
    ):
        """
        åˆå§‹åŒ–æ–‡æœ¬ LLM å¼•æ“

        Args:
            api_key: OpenAI API å¯†é’¥
            base_url: è‡ªå®šä¹‰ API Base URLï¼ˆå¦‚ yunwu å¹³å°ï¼‰
            model: æ¨¡å‹åç§°
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°
            callback_on_response_start: å“åº”å¼€å§‹å›è°ƒ
            callback_on_text_delta: æ–‡æœ¬å¢é‡å›è°ƒ (delta_text)
            callback_on_response_done: å“åº”å®Œæˆå›è°ƒ (full_text)
            callback_on_error: é”™è¯¯å›è°ƒ (error_message)
        """
        self.api_key = api_key
        self.model = model
        self.system_prompt = system_prompt
        self.temperature = temperature
        self.max_tokens = max_tokens

        # å¤„ç† base_url
        if not base_url.startswith("http"):
            base_url = f"https://{base_url}"
        if not base_url.endswith("/v1"):
            base_url = f"{base_url}/v1"

        self.base_url = base_url

        # å›è°ƒå‡½æ•°
        self.callback_on_response_start = callback_on_response_start
        self.callback_on_text_delta = callback_on_text_delta
        self.callback_on_response_done = callback_on_response_done
        self.callback_on_error = callback_on_error

        # åˆ›å»º OpenAI å®¢æˆ·ç«¯
        self.client = AsyncOpenAI(
            api_key=api_key,
            base_url=base_url
        )

        print(f"[TextLLM] åˆå§‹åŒ–æˆåŠŸ")
        print(f"[TextLLM] Base URL: {base_url}")
        print(f"[TextLLM] æ¨¡å‹: {model}")
        print(f"[TextLLM] Temperature: {temperature}")
        print(f"[TextLLM] Max Tokens: {max_tokens}")

    def update_system_prompt(self, new_prompt: str):
        """æ›´æ–°ç³»ç»Ÿæç¤ºè¯"""
        self.system_prompt = new_prompt
        print(f"[TextLLM] System Prompt å·²æ›´æ–°: {new_prompt[:100]}...")

    async def chat(
        self,
        user_message: str,
        conversation_history: Optional[List[Dict[str, str]]] = None,
        stream: bool = True
    ) -> str:
        """
        å‘é€æ¶ˆæ¯å¹¶æ¥æ”¶æ–‡æœ¬å“åº”

        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            conversation_history: å¯¹è¯å†å² [{"role": "user/assistant", "content": "..."}]
            stream: æ˜¯å¦æµå¼è¾“å‡º

        Returns:
            å®Œæ•´çš„ AI å›å¤æ–‡æœ¬
        """
        try:
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = []

            # æ·»åŠ  system prompt
            if self.system_prompt:
                messages.append({
                    "role": "system",
                    "content": self.system_prompt
                })

            # æ·»åŠ å†å²å¯¹è¯
            if conversation_history:
                messages.extend(conversation_history)

            # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
            messages.append({
                "role": "user",
                "content": user_message
            })

            print(f"[TextLLM] å‘é€æ¶ˆæ¯: {user_message[:50]}...")
            print(f"[TextLLM] æ¶ˆæ¯æ€»æ•°: {len(messages)} æ¡")

            # è°ƒç”¨ API
            if stream:
                return await self._chat_streaming(messages)
            else:
                return await self._chat_non_streaming(messages)

        except Exception as e:
            error_msg = f"TextLLM é”™è¯¯: {str(e)}"
            print(f"[TextLLM] {error_msg}")

            # å›è°ƒï¼šé”™è¯¯
            if self.callback_on_error:
                self.callback_on_error(error_msg)

            import traceback
            traceback.print_exc()
            return ""

    async def _chat_streaming(self, messages: List[Dict[str, str]]) -> str:
        """æµå¼å¯¹è¯"""
        full_response = ""
        first_chunk = True

        stream = await self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            stream=True,
            temperature=self.temperature,
            max_tokens=self.max_tokens
        )

        async for chunk in stream:
            if chunk.choices[0].delta.content:
                delta = chunk.choices[0].delta.content
                full_response += delta

                if first_chunk:
                    first_chunk = False
                    # å›è°ƒï¼šå“åº”å¼€å§‹
                    if self.callback_on_response_start:
                        self.callback_on_response_start()

                # å›è°ƒï¼šæ–‡æœ¬å¢é‡
                if self.callback_on_text_delta:
                    self.callback_on_text_delta(delta)

        print(f"[TextLLM] å“åº”å®Œæˆ: {len(full_response)} å­—ç¬¦")

        # å›è°ƒï¼šå“åº”å®Œæˆ
        if self.callback_on_response_done:
            self.callback_on_response_done(full_response)

        return full_response

    async def _chat_non_streaming(self, messages: List[Dict[str, str]]) -> str:
        """éæµå¼å¯¹è¯"""
        # å›è°ƒï¼šå“åº”å¼€å§‹
        if self.callback_on_response_start:
            self.callback_on_response_start()

        response = await self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            stream=False,
            temperature=self.temperature,
            max_tokens=self.max_tokens
        )

        full_response = response.choices[0].message.content

        print(f"[TextLLM] å“åº”å®Œæˆ: {len(full_response)} å­—ç¬¦")

        # å›è°ƒï¼šå“åº”å®Œæˆ
        if self.callback_on_response_done:
            self.callback_on_response_done(full_response)

        return full_response

    async def analyze_with_context(
        self,
        user_question: str,
        conversation_history: List[Dict[str, str]],
        background_data: Dict,
        personal_memo: str
    ) -> Dict:
        """
        ä½¿ç”¨èƒŒæ™¯æ•°æ®å’Œä¸ªäººå¤‡å¿˜å½•è¿›è¡Œæ·±åº¦åˆ†æï¼ˆå¤§æ¨¡å‹ä¸“ç”¨ï¼‰

        Args:
            user_question: ç”¨æˆ·é—®é¢˜
            conversation_history: å¯¹è¯å†å²
            background_data: èƒŒæ™¯æ•°æ®å­—å…¸
            personal_memo: ä¸ªäººå¤‡å¿˜å½•

        Returns:
            åˆ†æç»“æœå­—å…¸ {"answer": str, "analysis": str}
        """
        try:
            # æ„å»ºå¢å¼ºçš„ system prompt
            enhanced_prompt = f"""{self.system_prompt}

ã€èƒŒæ™¯æ•°æ®ã€‘
{self._format_background_data(background_data)}

ã€ç”¨æˆ·ä¸ªäººå¤‡å¿˜å½•ã€‘
{personal_memo}

ã€ä»»åŠ¡ã€‘
åŸºäºä»¥ä¸ŠèƒŒæ™¯æ•°æ®å’Œç”¨æˆ·å¤‡å¿˜å½•ï¼Œæ·±å…¥åˆ†æç”¨æˆ·çš„é—®é¢˜ï¼Œæä¾›ä¸“ä¸šã€è¯¦ç»†çš„å›ç­”ã€‚
"""

            # ä¸´æ—¶ä¿å­˜åŸ prompt
            original_prompt = self.system_prompt
            self.system_prompt = enhanced_prompt

            # è°ƒç”¨å¯¹è¯
            answer = await self.chat(
                user_message=user_question,
                conversation_history=conversation_history,
                stream=True
            )

            # æ¢å¤åŸ prompt
            self.system_prompt = original_prompt

            return {
                "answer": answer,
                "analysis": "æ·±åº¦åˆ†æå®Œæˆ"
            }

        except Exception as e:
            print(f"[TextLLM] æ·±åº¦åˆ†æé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return {
                "answer": "",
                "analysis": f"åˆ†æå¤±è´¥: {str(e)}"
            }

    def _format_background_data(self, data: Dict) -> str:
        """æ ¼å¼åŒ–èƒŒæ™¯æ•°æ®"""
        formatted = []
        for key, value in data.items():
            if isinstance(value, dict):
                formatted.append(f"\n## {key}")
                for sub_key, sub_value in value.items():
                    formatted.append(f"- {sub_key}: {sub_value}")
            else:
                formatted.append(f"\n## {key}")
                formatted.append(str(value))

        return "\n".join(formatted)


# æµ‹è¯•ä»£ç 
async def test_text_llm_engine():
    """æµ‹è¯• Text LLM Engine"""

    # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
    api_key = os.getenv("OPENAI_API_KEY")
    base_url = "https://yunwu.zeabur.app"

    if not api_key:
        print("è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        return

    # å®šä¹‰å›è°ƒå‡½æ•°
    def on_response_start():
        print("ğŸ¤– AI å¼€å§‹å›å¤...")

    def on_text_delta(delta):
        print(delta, end="", flush=True)

    def on_response_done(full_text):
        print(f"\nâœ“ å›å¤å®Œæˆ: {len(full_text)} å­—ç¬¦")

    # åˆ›å»ºå¼•æ“
    engine = TextLLMEngine(
        api_key=api_key,
        base_url=base_url,
        model="gpt-4o",
        system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯é¡¾é—®ã€‚",
        temperature=0.7,
        max_tokens=2000,
        callback_on_response_start=on_response_start,
        callback_on_text_delta=on_text_delta,
        callback_on_response_done=on_response_done
    )

    # å¯¹è¯æµ‹è¯•
    print("\n=== æµ‹è¯• 1: ç®€å•å¯¹è¯ ===")
    response1 = await engine.chat("ä»€ä¹ˆæ˜¯ TEMï¼ˆå¨èƒä¸å·®é”™ç®¡ç†ï¼‰ï¼Ÿ")

    print("\n\n=== æµ‹è¯• 2: å¸¦å†å²çš„å¯¹è¯ ===")
    history = [
        {"role": "user", "content": "ä»€ä¹ˆæ˜¯ TEMï¼ˆå¨èƒä¸å·®é”™ç®¡ç†ï¼‰ï¼Ÿ"},
        {"role": "assistant", "content": response1}
    ]
    await engine.chat("åœ¨èˆªç©ºé¢†åŸŸå¦‚ä½•åº”ç”¨ï¼Ÿ", conversation_history=history)

    print("\n\n=== æµ‹è¯• 3: æ·±åº¦åˆ†æ ===")
    background_data = {
        "èˆªç­ä¿¡æ¯": {
            "èˆªç­å·": "CA1234",
            "æœºå‹": "A320",
            "ç›®çš„åœ°": "åŒ—äº¬"
        },
        "å¤©æ°”": "ç›®çš„åœ°æœ‰é›¾"
    }
    personal_memo = "æˆ‘å…³æ³¨çš„æ˜¯è·‘é“è§†ç¨‹é—®é¢˜"

    result = await engine.analyze_with_context(
        user_question="è¿™æ¬¡èˆªç­æœ‰ä»€ä¹ˆé£é™©ï¼Ÿ",
        conversation_history=history,
        background_data=background_data,
        personal_memo=personal_memo
    )

    print(f"\nåˆ†æç»“æœ: {result['answer']}")


if __name__ == "__main__":
    import asyncio
    asyncio.run(test_text_llm_engine())
