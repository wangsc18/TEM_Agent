#!/usr/bin/env python3
"""
è¯­éŸ³äº¤äº’å¼•æ“ - æ•´åˆSTTã€LLMã€TTSå’ŒåŒæ¨¡å‹ç®¡ç†
"""
import os
import time
import asyncio
import tempfile
import threading
from typing import Optional, Literal
from datetime import datetime
import re
import wave
import subprocess
import base64

import numpy as np
import sounddevice as sd
from openai import AsyncOpenAI

from engines.dual_model_manager import DualModelManager


class VoiceInteractionEngine:
    """è¯­éŸ³äº¤äº’å¼•æ“ - ç”¨äºTEMæ¨¡æ‹Ÿå™¨"""

    def __init__(
        self,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        azure_endpoint: Optional[str] = None,
        azure_api_key: Optional[str] = None,
        azure_realtime_deployment: Optional[str] = None,
        small_model: str = "gpt-4o-mini",
        big_model: str = "gpt-4o",
        tts_engine: Literal["local", "edge", "openai"] = "local",
        voice_mode: Literal["audio", "text", "realtime"] = "text",
        audio_voice: str = "alloy",
        audio_format: str = "wav",
        callback_on_user_text=None,
        callback_on_ai_text=None,
        callback_on_ai_text_streaming=None,
        callback_on_status=None,
        callback_on_big_model_triggered=None,
        callback_on_tts_progress=None,
        enable_dual_model: bool = True
    ):
        """
        åˆå§‹åŒ–è¯­éŸ³äº¤äº’å¼•æ“

        Args:
            api_key: OpenAI APIå¯†é’¥
            base_url: è‡ªå®šä¹‰API Base URLï¼ˆç”¨äºç¬¬ä¸‰æ–¹å¹³å°ï¼‰
            azure_endpoint: Azure OpenAIç«¯ç‚¹ï¼ˆç”¨äºRealtime APIï¼‰
            azure_api_key: Azure OpenAI APIå¯†é’¥
            azure_realtime_deployment: Azure Realtimeéƒ¨ç½²åç§°
            small_model: å°æ¨¡å‹ï¼ˆå¿«é€Ÿå“åº”ï¼‰
            big_model: å¤§æ¨¡å‹ï¼ˆæ·±åº¦åˆ†æï¼‰
            tts_engine: TTSå¼•æ“ï¼ˆtextæ¨¡å¼æ—¶ä½¿ç”¨ï¼‰
            voice_mode: è¯­éŸ³æ¨¡å¼ ("audio"=ç›´æ¥éŸ³é¢‘è¾“å‡º, "text"=æ–‡æœ¬â†’TTS, "realtime"=æµå¼éŸ³é¢‘)
            audio_voice: éŸ³é¢‘è¯­éŸ³ï¼ˆaudio/realtimeæ¨¡å¼æ—¶ä½¿ç”¨ï¼‰
            audio_format: éŸ³é¢‘æ ¼å¼ï¼ˆaudioæ¨¡å¼æ—¶ä½¿ç”¨ï¼‰
            enable_dual_model: æ˜¯å¦å¯ç”¨åŒæ¨¡å‹æ¶æ„
            callback_on_user_text: å½“è¯†åˆ«åˆ°ç”¨æˆ·è¯­éŸ³æ—¶çš„å›è°ƒ (user_text)
            callback_on_ai_text: å½“AIç”Ÿæˆå®Œæ•´å›å¤æ—¶çš„å›è°ƒ (ai_text)
            callback_on_ai_text_streaming: å½“AIæµå¼ç”Ÿæˆæ—¶çš„å›è°ƒ (partial_text)
            callback_on_status: çŠ¶æ€æ›´æ–°å›è°ƒ (status_text, status_type)
            callback_on_big_model_triggered: å½“å¤§æ¨¡å‹è¢«è§¦å‘æ—¶çš„å›è°ƒ
            callback_on_tts_progress: TTSè¿›åº¦å›è°ƒ (sentence, current_index, total_count, status)
        """
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("æœªæ‰¾åˆ° OPENAI_API_KEY")

        # Azure Realtime API é…ç½®
        self.azure_endpoint = azure_endpoint
        self.azure_api_key = azure_api_key
        self.azure_realtime_deployment = azure_realtime_deployment or "gpt-realtime-mini"
        self.use_realtime_api = False  # æ ‡å¿—ä½ï¼Œç¨åæ ¹æ®é…ç½®è®¾ç½®

        # å¤„ç†è‡ªå®šä¹‰ Base URLï¼ˆå‚è€ƒ test_audio_model_simple.pyï¼‰
        if base_url:
            # ç¡®ä¿æœ‰åè®®å‰ç¼€
            if not base_url.startswith("http"):
                base_url = f"https://{base_url}"
            # ç¡®ä¿æœ‰ /v1 è·¯å¾„
            if not base_url.endswith("/v1"):
                base_url = f"{base_url}/v1"
            print(f"[APIé…ç½®] ä½¿ç”¨è‡ªå®šä¹‰Base URL: {base_url}")

        self.client = AsyncOpenAI(
            api_key=self.api_key,
            base_url=base_url  # ä¼ å…¥ base_url
        )
        self.small_model = small_model
        self.big_model = big_model
        self.tts_engine = tts_engine
        self.voice_mode = voice_mode
        self.audio_voice = audio_voice
        self.audio_format = audio_format

        self.callback_on_user_text = callback_on_user_text
        self.callback_on_ai_text = callback_on_ai_text
        self.callback_on_ai_text_streaming = callback_on_ai_text_streaming
        self.callback_on_status = callback_on_status
        self.callback_on_tts_progress = callback_on_tts_progress

        # åŒæ¨¡å‹ç®¡ç†å™¨
        self.enable_dual_model = enable_dual_model
        if enable_dual_model:
            self.dual_model_manager = DualModelManager(
                self.client,
                small_model,
                big_model,
                callback_on_big_model_triggered=callback_on_big_model_triggered
            )
        else:
            self.dual_model_manager = None

        # TTSéŸ³é¢‘ä¿å­˜ç›®å½•
        self.tts_audio_dir = os.path.join(os.getcwd(), "tts_audio_debug")
        os.makedirs(self.tts_audio_dir, exist_ok=True)

        # æ ¹æ®è¯­éŸ³æ¨¡å¼åˆå§‹åŒ–
        if self.voice_mode == "realtime":
            # æ£€æŸ¥ Azure Realtime API é…ç½®
            if self.azure_endpoint and self.azure_api_key:
                self.use_realtime_api = True
                print(f"[è¯­éŸ³å¼•æ“] æ¨¡å¼: Realtimeæµå¼éŸ³é¢‘ (Azure OpenAI)")
                print(f"[Realtimeé…ç½®] ç«¯ç‚¹: {self.azure_endpoint}")
                print(f"[Realtimeé…ç½®] éƒ¨ç½²: {self.azure_realtime_deployment}")
                print(f"[Realtimeé…ç½®] è¯­éŸ³: {self.audio_voice}")

                # åˆå§‹åŒ– sounddevice éŸ³é¢‘æµ
                self.audio_sample_rate = 24000
                self.audio_channels = 1
                self.audio_dtype = np.int16

                try:
                    # åˆ›å»ºæŒä¹…çš„éŸ³é¢‘è¾“å‡ºæµ
                    self.audio_stream = sd.OutputStream(
                        samplerate=self.audio_sample_rate,
                        channels=self.audio_channels,
                        dtype=self.audio_dtype,
                        blocksize=4096
                    )
                    self.audio_stream.start()
                    print(f"[éŸ³é¢‘æµ] åˆå§‹åŒ–æˆåŠŸ: {self.audio_sample_rate}Hz, {self.audio_channels}é€šé“, 16-bit PCM")
                except Exception as e:
                    print(f"[éŸ³é¢‘æµ] åˆå§‹åŒ–å¤±è´¥: {e}")
                    self.audio_stream = None
            else:
                print(f"[è¯­éŸ³å¼•æ“] âš ï¸ realtime æ¨¡å¼éœ€è¦ Azure é…ç½®ï¼Œå›é€€åˆ° audio æ¨¡å¼")
                self.voice_mode = "audio"
                self.use_realtime_api = False

        if self.voice_mode == "audio":
            print(f"[è¯­éŸ³å¼•æ“] æ¨¡å¼: Audioç›´å‡º (è·³è¿‡TTS)")
            print(f"[è¯­éŸ³å¼•æ“] éŸ³é¢‘è¯­éŸ³: {self.audio_voice}")
            print(f"[è¯­éŸ³å¼•æ“] éŸ³é¢‘æ–‡ä»¶ä¿å­˜è·¯å¾„: {self.tts_audio_dir}")
        elif self.voice_mode == "text":
            print(f"[è¯­éŸ³å¼•æ“] æ¨¡å¼: æ–‡æœ¬â†’TTS")
            print(f"[TTSè°ƒè¯•] éŸ³é¢‘æ–‡ä»¶ä¿å­˜è·¯å¾„: {self.tts_audio_dir}")
            # TTSéŸ³é¢‘è£å‰ªè®¾ç½®ï¼ˆå»é™¤ç»“å°¾é™éŸ³ï¼‰
            self.trim_end_silence_ms = 200  # è£å‰ªç»“å°¾200ms
            print(f"[TTSä¼˜åŒ–] è‡ªåŠ¨è£å‰ªç»“å°¾é™éŸ³: {self.trim_end_silence_ms}ms")

        # å½•éŸ³å‚æ•°
        self.sample_rate = 16000
        self.max_recording_duration = 10
        self.silence_threshold = 1.5
        self.silence_duration_to_stop = 0.02

        # å¯¹è¯å†å² - TEMåœºæ™¯ä¸“ç”¨promptï¼ˆä¸¥æ ¼é™åˆ¶å¹»è§‰ï¼‰
        self.base_system_prompt = """ä½ æ˜¯ä¸€åèˆªç©ºé£è¡Œå‘˜AIä¼™ä¼´ï¼Œæ­£åœ¨ä¸å¦ä¸€åé£è¡Œå‘˜è¿›è¡ŒTEMï¼ˆå¨èƒä¸å·®é”™ç®¡ç†ï¼‰æ¡ˆä¾‹è®¨è®ºã€‚

ã€æ ¸å¿ƒåŸåˆ™ - å®‰å…¨ç¬¬ä¸€ã€‘
åœ¨èˆªç©ºé¢†åŸŸï¼Œå‡†ç¡®æ€§å’Œå®‰å…¨æ€§é«˜äºä¸€åˆ‡ã€‚ä½ å¿…é¡»ä¸¥æ ¼éµå®ˆä»¥ä¸‹è§„åˆ™ï¼š

âŒ ç»å¯¹ç¦æ­¢çš„è¡Œä¸ºï¼š
1. ç¼–é€ æˆ–çŒœæµ‹ä¸“ä¸šæ•°æ®ï¼ˆå¦‚MELæ¡æ¬¾ã€æ€§èƒ½æ•°æ®ã€æ³•è§„æ¡æ¬¾ï¼‰
2. å¯¹ä¸ç¡®å®šçš„ä¸“ä¸šé—®é¢˜ç»™å‡ºæ¨¡ç³Šæˆ–çŒœæµ‹æ€§çš„å›ç­”
3. å›ç­”è¶…å‡ºä½ å½“å‰ä¿¡æ¯èŒƒå›´çš„ä¸“ä¸šé—®é¢˜

âœ… å¿…é¡»éµå®ˆçš„è¡Œä¸ºï¼š
1. å¦‚æœé—®é¢˜æ¶‰åŠå…·ä½“çš„ä¸“ä¸šçŸ¥è¯†ï¼ˆMELã€è¿è¡Œæ‰‹å†Œã€æ³•è§„ã€æ€§èƒ½è®¡ç®—ç­‰ï¼‰ï¼Œä½ å¿…é¡»ç«‹å³å›å¤"è®©æˆ‘æŸ¥æ‰¾ä¸€ä¸‹ç›¸å…³ä¿¡æ¯"
2. åªè®¨è®ºä½ å·²çŸ¥çš„ã€ç¡®å®šçš„ä¿¡æ¯
3. ç”¨å£è¯­åŒ–ã€ç®€çŸ­çš„æ–¹å¼äº¤æµï¼ˆ10-20å­—ï¼‰
4. é€‚å½“ä½¿ç”¨"å—¯"ã€"å¥½çš„"ç­‰å£è¯­åŒ–è¡¨è¾¾

ã€ä½•æ—¶å¿…é¡»è¯´"è®©æˆ‘æŸ¥æ‰¾ä¸€ä¸‹ç›¸å…³ä¿¡æ¯"ã€‘
- ç”¨æˆ·é—®åˆ°å…·ä½“çš„MELæ¡æ¬¾ã€é™åˆ¶ã€æ‰‹å†Œæ¡æ¬¾
- æ¶‰åŠæ€§èƒ½è®¡ç®—ã€é‡é‡é™åˆ¶ç­‰å…·ä½“æ•°å€¼
- éœ€è¦å¼•ç”¨æ³•è§„ã€è¿è¡Œæ ‡å‡†
- ä»»ä½•ä½ ä¸100%ç¡®å®šçš„ä¸“ä¸šé—®é¢˜

ã€ä½ å¯ä»¥ç›´æ¥å›ç­”çš„ã€‘
- ä¸€èˆ¬æ€§çš„å¨èƒè¯†åˆ«å’Œè®¨è®º
- åŸºäºå·²çŸ¥ä¿¡æ¯çš„è§‚å¯Ÿå’Œæ€»ç»“
- å¯¹è¯å¼•å¯¼å’Œç¡®è®¤æ€§é—®é¢˜

ç¤ºä¾‹å¯¹è¯ï¼š
ç”¨æˆ·: "APUæ•…éšœä¼šå½±å“èµ·é£å—ï¼Ÿ"
âŒ é”™è¯¯: "ä¼šçš„ï¼Œéœ€è¦å»¶é•¿èµ·é£æ»‘è·‘è·ç¦»ï¼Œå¤§çº¦å¢åŠ 10%ã€‚"ï¼ˆç¼–é€ æ•°æ®ï¼‰
âœ… æ­£ç¡®: "å—¯ï¼Œè®©æˆ‘æŸ¥æ‰¾ä¸€ä¸‹ç›¸å…³ä¿¡æ¯ã€‚"ï¼ˆè§¦å‘ä¸“å®¶åˆ†æï¼‰

ç”¨æˆ·: "ä½ è§‰å¾—è¿™æ¬¡é£è¡Œæœ‰å“ªäº›æ½œåœ¨å¨èƒï¼Ÿ"
âœ… æ­£ç¡®: "æˆ‘çœ‹åˆ°å‡ ä¸ªå¨èƒï¼Œç›®çš„åœ°æœ‰é›¾ï¼Œè·‘é“ç¼©çŸ­ï¼Œè¿˜æœ‰APUæ•…éšœã€‚"ï¼ˆåŸºäºå·²çŸ¥ä¿¡æ¯ï¼‰

è®°ä½ï¼šå®å¯è°¨æ…åœ°è¯´"è®©æˆ‘æŸ¥æ‰¾ä¸€ä¸‹"ï¼Œä¹Ÿä¸è¦ç»™å‡ºä¸ç¡®å®šçš„ä¸“ä¸šå»ºè®®ï¼"""

        self.conversation_history = [
            {
                "role": "system",
                "content": self.base_system_prompt
            }
        ]

        # ç”¨äºåœ¨åå°çº¿ç¨‹è¿è¡Œå¼‚æ­¥ä»»åŠ¡
        self.loop = None
        self.recording = False
        self.current_audio_data = []

        # èƒŒæ™¯æ•°æ®å’Œç”¨æˆ·å¤‡å¿˜å½•ï¼ˆç”¨äºå¤§æ¨¡å‹åˆ†æï¼‰
        self.background_data = {}
        self.personal_memo = ""

    def set_background_data(self, data: dict):
        """è®¾ç½®èƒŒæ™¯æ•°æ®ï¼ˆç”¨äºå¤§æ¨¡å‹ï¼‰"""
        self.background_data = data

    def set_personal_memo(self, memo: str):
        """è®¾ç½®ç”¨æˆ·ä¸ªäººå¤‡å¿˜å½•ï¼ˆç”¨äºå¤§æ¨¡å‹ï¼‰"""
        self.personal_memo = memo

    def _update_status(self, text: str, status_type: str = "info"):
        """æ›´æ–°çŠ¶æ€ï¼ˆåœ¨ä¸»çº¿ç¨‹è°ƒç”¨å›è°ƒï¼‰"""
        if self.callback_on_status:
            self.callback_on_status(text, status_type)

    def _on_user_text_recognized(self, text: str):
        """ç”¨æˆ·è¯­éŸ³è¯†åˆ«å®Œæˆ"""
        if self.callback_on_user_text:
            self.callback_on_user_text(text)

    def _on_ai_response(self, text: str):
        """AIå›å¤ç”Ÿæˆå®Œæˆ"""
        if self.callback_on_ai_text:
            self.callback_on_ai_text(text)

    def _on_ai_response_streaming(self, text: str):
        """AIæµå¼ç”Ÿæˆä¸­ï¼ˆæ¯ç”Ÿæˆä¸€ä¸ªå¥å­è°ƒç”¨ï¼‰"""
        if self.callback_on_ai_text_streaming:
            self.callback_on_ai_text_streaming(text)

    def _on_tts_progress(self, sentence: str, index: int, status: str):
        """TTSè¿›åº¦æ›´æ–°ï¼ˆsentence: å¥å­å†…å®¹, index: åºå·, status: çŠ¶æ€ï¼‰"""
        if self.callback_on_tts_progress:
            self.callback_on_tts_progress(sentence, index, status)

    def start_recording(self):
        """å¼€å§‹å½•éŸ³ï¼ˆä»…å½•éŸ³+STTï¼Œä¸è§¦å‘LLMï¼‰"""
        def run_async_recording():
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            self.loop = loop
            loop.run_until_complete(self._async_record_and_transcribe())

        thread = threading.Thread(target=run_async_recording, daemon=True)
        thread.start()

    async def _async_record_and_transcribe(self):
        """å¼‚æ­¥å½•éŸ³å¹¶è½¬æ–‡å­—ï¼ˆä»…STTï¼Œä¸è°ƒç”¨LLMï¼‰"""
        try:
            # 1. å½•éŸ³
            self._update_status("ğŸ¤ æ­£åœ¨å½•éŸ³...", "recording")
            audio_data = await self._record_audio()

            if audio_data is None:
                self._update_status("âŒ å½•éŸ³å¤±è´¥", "error")
                return

            # 2. è¯­éŸ³è¯†åˆ«
            self._update_status("ğŸ”„ è¯­éŸ³è¯†åˆ«ä¸­...", "processing")
            user_text = await self._speech_to_text(audio_data)

            if not user_text:
                self._update_status("âŒ æœªè¯†åˆ«åˆ°è¯­éŸ³", "error")
                return

            # 3. å°†è¯†åˆ«ç»“æœå¡«å……åˆ°è¾“å…¥æ¡†ï¼ˆä¸è‡ªåŠ¨å‘é€ï¼‰
            self._on_user_text_recognized(user_text)
            self._update_status("âœ“ è¯†åˆ«å®Œæˆï¼Œè¯·ç¡®è®¤åå‘é€", "success")

        except Exception as e:
            self._update_status(f"âŒ é”™è¯¯: {str(e)}", "error")

    def process_user_message(self, user_message: str):
        """å¤„ç†ç”¨æˆ·æ¶ˆæ¯ï¼ˆLLMå¯¹è¯+TTSï¼‰- æ”¯æŒåŒæ¨¡å‹"""
        def run_async_processing():
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            self.loop = loop
            loop.run_until_complete(self._async_llm_and_tts(user_message))

        thread = threading.Thread(target=run_async_processing, daemon=True)
        thread.start()

    async def _async_llm_and_tts(self, user_message: str):
        """å¼‚æ­¥LLMç”Ÿæˆå’ŒTTSæ’­æ”¾ - æ•´åˆåŒæ¨¡å‹é€»è¾‘"""
        try:
            # æ ¹æ®è¯­éŸ³æ¨¡å¼é€‰æ‹©ä¸åŒçš„å¤„ç†è·¯å¾„
            if self.voice_mode == "realtime" and self.use_realtime_api:
                await self._async_llm_with_realtime_audio(user_message)
            elif self.voice_mode == "audio":
                await self._async_llm_with_audio(user_message)
            else:
                await self._async_llm_with_text_tts(user_message)

        except Exception as e:
            self._update_status(f"âŒ é”™è¯¯: {str(e)}", "error")
            print(f"LLMå¤„ç†é”™è¯¯: {e}")

    async def _async_llm_with_audio(self, user_message: str):
        """Audioæ¨¡å¼ï¼šä½¿ç”¨ gpt-4o-audio-preview ç›´æ¥è¾“å‡ºéŸ³é¢‘ï¼ˆè·³è¿‡TTSï¼‰"""
        try:
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
            self.conversation_history.append({
                "role": "user",
                "content": user_message
            })

            # 1. æ›´æ–°ç³»ç»Ÿæç¤ºï¼ˆå¦‚æœæœ‰ç­–ç•¥æ›´æ–°ï¼‰
            if self.enable_dual_model and self.dual_model_manager:
                enhanced_prompt = self.dual_model_manager.get_enhanced_system_prompt(
                    self.base_system_prompt
                )
                self.conversation_history[0]["content"] = enhanced_prompt

            # 2. è°ƒç”¨Audio APIï¼ˆéæµå¼ï¼‰
            self._update_status("ğŸ¤– AIæ€è€ƒä¸­...", "processing")

            # æ‰“å°è¯·æ±‚å‚æ•°ï¼ˆè°ƒè¯•ç”¨ï¼‰
            print(f"[Audioè¯·æ±‚] æ¨¡å‹: {self.small_model}")
            print(f"[Audioè¯·æ±‚] æ¨¡æ€: ['text', 'audio']")
            print(f"[Audioè¯·æ±‚] è¯­éŸ³: {self.audio_voice}")
            print(f"[Audioè¯·æ±‚] æ ¼å¼: {self.audio_format}")

            response = await self.client.chat.completions.create(
                model=self.small_model,  # gpt-4o-audio-preview
                modalities=["text", "audio"],
                audio={
                    "voice": self.audio_voice,
                    "format": self.audio_format
                },
                messages=self.conversation_history
            )

            # è°ƒè¯•ï¼šæŸ¥çœ‹å“åº”ç»“æ„
            print(f"[Audioè°ƒè¯•] å“åº”ç±»å‹: {type(response)}")
            print(f"[Audioè°ƒè¯•] æ˜¯å¦æœ‰choices: {hasattr(response, 'choices')}")
            if hasattr(response, 'choices') and len(response.choices) > 0:
                choice = response.choices[0]
                print(f"[Audioè°ƒè¯•] æ˜¯å¦æœ‰message: {hasattr(choice, 'message')}")
                if hasattr(choice, 'message'):
                    print(f"[Audioè°ƒè¯•] æ˜¯å¦æœ‰content: {hasattr(choice.message, 'content')}")
                    print(f"[Audioè°ƒè¯•] contentå€¼: {choice.message.content}")
                    print(f"[Audioè°ƒè¯•] æ˜¯å¦æœ‰audio: {hasattr(choice.message, 'audio')}")
                    if hasattr(choice.message, 'audio'):
                        print(f"[Audioè°ƒè¯•] audioå€¼: {choice.message.audio}")
                        # æ›´è¯¦ç»†çš„audioè°ƒè¯•
                        if choice.message.audio is not None:
                            print(f"[Audioè°ƒè¯•] audioç±»å‹: {type(choice.message.audio)}")
                            print(f"[Audioè°ƒè¯•] audioå±æ€§: {dir(choice.message.audio)}")
                        else:
                            print(f"[Audioè°ƒè¯•] âš ï¸ audioä¸ºNone! å¯èƒ½åŸå› :")
                            print(f"            1. å¹³å°ä¸å®Œå…¨æ”¯æŒaudioæ¨¡æ€")
                            print(f"            2. è¯·æ±‚å‚æ•°ä¸æ­£ç¡®")
                            print(f"            3. APIå†…å®¹ç­–ç•¥é™åˆ¶")

                    # æ‰“å°å®Œæ•´å“åº”ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                    print(f"[Audioè°ƒè¯•] å®Œæ•´choice.messageå¯¹è±¡: {choice.message}")
                    print(f"[Audioè°ƒè¯•] finish_reason: {choice.finish_reason}")

            # 3. æå–å“åº”
            if isinstance(response, str):
                # å¦‚æœè¿”å›çš„æ˜¯å­—ç¬¦ä¸²ï¼ˆæŸäº›å¹³å°å¯èƒ½ä¸æ”¯æŒaudioæ¨¡å¼ï¼‰
                print(f"[Audioæ¨¡å¼] âš ï¸ å¹³å°è¿”å›å­—ç¬¦ä¸²è€ŒééŸ³é¢‘å¯¹è±¡ï¼Œå¯èƒ½ä¸æ”¯æŒaudioæ¨¡å¼")
                full_response = response
                self._on_ai_response(full_response)

                # æ·»åŠ åˆ°å†å²
                self.conversation_history.append({
                    "role": "assistant",
                    "content": full_response
                })

                self._update_status("âš ï¸ å¹³å°å¯èƒ½ä¸æ”¯æŒaudioæ¨¡å¼ï¼Œå·²åˆ‡æ¢ä¸ºæ–‡æœ¬", "error")
                return

            choice = response.choices[0]
            full_response = ""

            # éŸ³é¢‘å“åº”
            if hasattr(choice.message, 'audio') and choice.message.audio:
                audio_data_base64 = choice.message.audio.data
                audio_transcript = choice.message.audio.transcript

                if audio_transcript:
                    # ä½¿ç”¨è½¬å½•æ–‡æœ¬ä½œä¸ºå›å¤å†…å®¹
                    full_response = audio_transcript
                    # æµå¼æ˜¾ç¤ºè½¬å½•æ–‡æœ¬åˆ°ç•Œé¢
                    self._on_ai_response_streaming(audio_transcript)
                    print(f"[Audioæ¨¡å¼] éŸ³é¢‘è½¬å½•: {audio_transcript}")

                # è§£ç éŸ³é¢‘
                audio_bytes = base64.b64decode(audio_data_base64)
                audio_array = np.frombuffer(audio_bytes, dtype=np.int16)

                # ä¿å­˜éŸ³é¢‘æ–‡ä»¶ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                timestamp = datetime.now().strftime("%H%M%S")
                filename = f"ai_response_{timestamp}.{self.audio_format}"
                audio_path = os.path.join(self.tts_audio_dir, filename)

                with wave.open(audio_path, 'wb') as wf:
                    wf.setnchannels(1)
                    wf.setsampwidth(2)  # 16-bit
                    wf.setframerate(24000)  # Audio APIä½¿ç”¨24kHz
                    wf.writeframes(audio_array.tobytes())

                print(f"[Audioæ¨¡å¼] å·²ä¿å­˜éŸ³é¢‘: {filename}")

                # æ’­æ”¾éŸ³é¢‘
                self._update_status("ğŸ”Š æ’­æ”¾AIè¯­éŸ³...", "speaking")
                await self._play_single_audio(audio_path)

            # æ–‡æœ¬å“åº”ï¼ˆå¦‚æœæœ‰çš„è¯ï¼Œä½œä¸ºå¤‡ç”¨æˆ–audioä¸ºNoneæ—¶ä½¿ç”¨ï¼‰
            elif choice.message.content:
                full_response = choice.message.content
                self._on_ai_response_streaming(full_response)
                print(f"[Audioæ¨¡å¼] âš ï¸ APIæœªè¿”å›éŸ³é¢‘ï¼Œå›é€€åˆ°æ–‡æœ¬æ¨¡å¼")
                print(f"[Audioæ¨¡å¼] AIæ–‡æœ¬å›å¤: {full_response}")

                # ã€æ–°å¢ã€‘å¦‚æœaudioä¸ºNoneï¼Œè‡ªåŠ¨ä½¿ç”¨TTSè½¬æ¢æ–‡æœ¬
                print(f"[Audioå›é€€] ä½¿ç”¨TTSå¼•æ“ç”ŸæˆéŸ³é¢‘...")
                self._update_status("ğŸ”Š TTSè½¬æ¢ä¸­...", "processing")

                # åˆ†å¥å¤„ç†ï¼ˆé¿å…ä¸€æ¬¡æ€§è½¬æ¢è¿‡é•¿æ–‡æœ¬ï¼‰
                sentences = self._extract_complete_sentences(full_response)
                if not sentences:  # å¦‚æœæ²¡æœ‰åˆ†å¥ï¼Œç›´æ¥ä½¿ç”¨å®Œæ•´æ–‡æœ¬
                    sentences = [full_response]

                for sentence in sentences:
                    audio_file = await self._quick_tts(sentence)
                    if audio_file:
                        self._update_status("ğŸ”Š æ’­æ”¾AIè¯­éŸ³...", "speaking")
                        await self._play_single_audio(audio_file)

                print(f"[Audioå›é€€] TTSè½¬æ¢å®Œæˆ")

            # æ·»åŠ AIå›å¤åˆ°å†å²ï¼ˆä½¿ç”¨æ–‡æœ¬ç‰ˆæœ¬ï¼‰
            self.conversation_history.append({
                "role": "assistant",
                "content": full_response
            })

            # 4. æ£€æŸ¥æ˜¯å¦è§¦å‘å¤§æ¨¡å‹ï¼ˆåŸºäºæ–‡æœ¬å“åº”ï¼‰
            should_trigger_big_model = False
            if (self.enable_dual_model and
                self.dual_model_manager and
                self.dual_model_manager.check_if_trigger_big_model(full_response)):
                should_trigger_big_model = True

            # å¦‚æœè§¦å‘äº†å¤§æ¨¡å‹ï¼Œåå°å¼‚æ­¥å¤„ç†
            if should_trigger_big_model:
                print("[Audioæ¨¡å¼] æ£€æµ‹åˆ°è§¦å‘è¯ï¼Œå¯åŠ¨å¤§æ¨¡å‹åˆ†æ...")
                self._update_status("ğŸ§  æ­£åœ¨æ·±åº¦åˆ†æ...", "processing")

                def run_big_model():
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    try:
                        loop.run_until_complete(self._process_with_big_model(user_message))
                    except Exception as e:
                        print(f"[å¤§æ¨¡å‹çº¿ç¨‹] é”™è¯¯: {e}")
                    finally:
                        loop.close()

                thread = threading.Thread(target=run_big_model, daemon=True)
                thread.start()
                print("[Audioæ¨¡å¼] å¤§æ¨¡å‹çº¿ç¨‹å·²å¯åŠ¨")

            self._update_status("âœ“ å®Œæˆ", "success")

        except Exception as e:
            self._update_status(f"âŒ é”™è¯¯: {str(e)}", "error")
            print(f"Audioæ¨¡å¼é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

    async def _async_llm_with_realtime_audio(self, user_message: str):
        """Realtimeæ¨¡å¼ï¼šä½¿ç”¨ Azure Realtime API æµå¼è¾“å‡ºéŸ³é¢‘ï¼ˆWebSocketï¼‰"""
        try:
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
            self.conversation_history.append({
                "role": "user",
                "content": user_message
            })

            # 1. æ›´æ–°ç³»ç»Ÿæç¤ºï¼ˆå¦‚æœæœ‰ç­–ç•¥æ›´æ–°ï¼‰
            if self.enable_dual_model and self.dual_model_manager:
                enhanced_prompt = self.dual_model_manager.get_enhanced_system_prompt(
                    self.base_system_prompt
                )
                current_system_prompt = enhanced_prompt
            else:
                current_system_prompt = self.base_system_prompt

            # è°ƒè¯•ï¼šæ‰“å° system prompt
            print(f"[Realtimeè°ƒè¯•] System Prompt (å‰100å­—): {current_system_prompt[:100]}...")
            print(f"[Realtimeè°ƒè¯•] å¯¹è¯å†å²é•¿åº¦: {len(self.conversation_history)} æ¡")

            # 2. åˆ›å»º Realtime API å®¢æˆ·ç«¯
            base_url = self.azure_endpoint.replace("https://", "wss://").rstrip("/") + "/openai/v1"

            realtime_client = AsyncOpenAI(
                websocket_base_url=base_url,
                api_key=self.azure_api_key
            )

            print(f"[Realtime] è¿æ¥åˆ°: {base_url}")
            print(f"[Realtime] éƒ¨ç½²: {self.azure_realtime_deployment}")

            # 3. åˆ›å»ºå¼‚æ­¥éŸ³é¢‘é˜Ÿåˆ—
            audio_queue = asyncio.Queue()

            # 4. å¯åŠ¨éŸ³é¢‘æ’­æ”¾åç¨‹
            async def audio_player():
                """åå°æ’­æ”¾éŸ³é¢‘é˜Ÿåˆ—ä¸­çš„æ•°æ®"""
                try:
                    while True:
                        audio_data = await audio_queue.get()

                        # None è¡¨ç¤ºé˜Ÿåˆ—ç»“æŸ
                        if audio_data is None:
                            break

                        # æ’­æ”¾éŸ³é¢‘ï¼ˆåœ¨ executor ä¸­æ‰§è¡Œï¼Œé¿å…é˜»å¡ï¼‰
                        if self.audio_stream:
                            try:
                                audio_array = np.frombuffer(audio_data, dtype=self.audio_dtype)
                                audio_array = audio_array.reshape(-1, 1)

                                # ä½¿ç”¨ run_in_executor åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œé˜»å¡æ“ä½œ
                                loop = asyncio.get_event_loop()
                                await loop.run_in_executor(
                                    None,
                                    self.audio_stream.write,
                                    audio_array
                                )
                            except Exception as e:
                                print(f"[Realtime] éŸ³é¢‘æ’­æ”¾é”™è¯¯: {e}")

                        audio_queue.task_done()
                except Exception as e:
                    print(f"[Realtime] éŸ³é¢‘æ’­æ”¾å™¨é”™è¯¯: {e}")

            play_task = asyncio.create_task(audio_player())

            # 5. è¿æ¥å¹¶é…ç½® session
            async with realtime_client.realtime.connect(
                model=self.azure_realtime_deployment,
            ) as connection:
                # é…ç½® sessionï¼ˆä¸è®¾ç½® instructionsï¼Œå› ä¸º Azure å¯èƒ½ä¸æ”¯æŒï¼‰
                await connection.session.update(session={
                    "output_modalities": ["audio"],
                    "audio": {
                        "input": {
                            "transcription": {
                                "model": "whisper-1",
                            },
                            "format": {
                                "type": "audio/pcm",
                                "rate": 24000,
                            },
                            "turn_detection": {
                                "type": "server_vad",
                                "threshold": 0.5,
                                "prefix_padding_ms": 300,
                                "silence_duration_ms": 200,
                                "create_response": True,
                            }
                        },
                        "output": {
                            "voice": self.audio_voice,
                            "format": {
                                "type": "audio/pcm",
                                "rate": 24000,
                            }
                        }
                    }
                })

                print(f"[Realtime] Session é…ç½®å®Œæˆ")

                # 6. å°† system prompt ä½œä¸ºç¬¬ä¸€æ¡æ¶ˆæ¯æ·»åŠ ï¼ˆæ›¿ä»£ instructionsï¼‰
                print(f"[Realtime] æ·»åŠ  system prompt ä½œä¸ºç¬¬ä¸€æ¡æ¶ˆæ¯")
                await connection.conversation.item.create(
                    item={
                        "type": "message",
                        "role": "system",
                        "content": [{"type": "input_text", "text": current_system_prompt}],
                    }
                )

                # 7. æ·»åŠ å†å²å¯¹è¯åˆ° conversationï¼ˆæ’é™¤ system æ¶ˆæ¯å’Œå½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼‰
                history_to_add = self.conversation_history[1:-1]

                if history_to_add:
                    print(f"[Realtime] æ·»åŠ  {len(history_to_add)} æ¡å†å²æ¶ˆæ¯åˆ° conversation")
                    for i, msg in enumerate(history_to_add):
                        print(f"[Realtimeè°ƒè¯•] å†å²[{i}]: role={msg['role']}, content={msg['content'][:50]}...")
                        await connection.conversation.item.create(
                            item={
                                "type": "message",
                                "role": msg["role"],
                                "content": [{"type": "input_text", "text": msg["content"]}],
                            }
                        )
                else:
                    print(f"[Realtime] æ— å†å²æ¶ˆæ¯éœ€è¦æ·»åŠ ")

                # 8. å‘é€å½“å‰ç”¨æˆ·æ¶ˆæ¯
                print(f"[Realtime] å‘é€ç”¨æˆ·æ¶ˆæ¯: {user_message[:30]}...")
                await connection.conversation.item.create(
                    item={
                        "type": "message",
                        "role": "user",
                        "content": [{"type": "input_text", "text": user_message}],
                    }
                )
                await connection.response.create()

                # 8. å®æ—¶æ¥æ”¶äº‹ä»¶å¹¶æ”¾å…¥é˜Ÿåˆ—
                self._update_status("ğŸ¤– AIæ€è€ƒä¸­...", "processing")
                self._on_ai_response_streaming("")  # å¼€å§‹æ–°çš„æµå¼å“åº”

                full_response = ""
                audio_chunk_count = 0

                async for event in connection:
                    if event.type == "response.output_text.delta":
                        # æ–‡æœ¬å¢é‡ï¼ˆé€šå¸¸ä¸ä¼šæœ‰ï¼Œå› ä¸ºoutput_modalitiesåªæœ‰audioï¼‰
                        pass

                    elif event.type == "response.output_audio.delta":
                        # éŸ³é¢‘æ•°æ®å— - æ”¾å…¥é˜Ÿåˆ—ï¼Œä¸é˜»å¡
                        audio_data = base64.b64decode(event.delta)
                        await audio_queue.put(audio_data)

                        audio_chunk_count += 1
                        if audio_chunk_count == 1:
                            # é¦–æ¬¡æ’­æ”¾æ—¶æ›´æ–°çŠ¶æ€
                            self._update_status("ğŸ”Š æ’­æ”¾AIè¯­éŸ³...", "speaking")

                    elif event.type == "response.output_audio_transcript.delta":
                        # éŸ³é¢‘è½¬å½•æ–‡æœ¬ï¼ˆæµå¼ï¼‰
                        delta_text = event.delta
                        full_response += delta_text
                        self._on_ai_response_streaming(delta_text)

                    elif event.type == "response.output_audio_transcript.done":
                        # éŸ³é¢‘è½¬å½•å®Œæˆ
                        print(f"[Realtime] è½¬å½•å®Œæˆ: {full_response[:50]}...")

                    elif event.type == "response.done":
                        # å“åº”å®Œæˆ
                        print(f"[Realtime] å“åº”å®Œæˆï¼Œå…±æ¥æ”¶ {audio_chunk_count} ä¸ªéŸ³é¢‘å—")
                        break

            # 9. æ ‡è®°éŸ³é¢‘é˜Ÿåˆ—ç»“æŸå¹¶ç­‰å¾…æ’­æ”¾å®Œæˆ
            await audio_queue.put(None)
            await play_task
            print(f"[Realtime] éŸ³é¢‘æ’­æ”¾å®Œæˆ")

            # 10. æ·»åŠ AIå›å¤åˆ°å†å²ï¼ˆä½¿ç”¨è½¬å½•æ–‡æœ¬ï¼‰
            self.conversation_history.append({
                "role": "assistant",
                "content": full_response
            })

            # 11. æ£€æŸ¥æ˜¯å¦è§¦å‘å¤§æ¨¡å‹
            should_trigger_big_model = False
            if (self.enable_dual_model and
                self.dual_model_manager and
                self.dual_model_manager.check_if_trigger_big_model(full_response)):
                should_trigger_big_model = True

            # å¦‚æœè§¦å‘äº†å¤§æ¨¡å‹ï¼Œåå°å¼‚æ­¥å¤„ç†
            if should_trigger_big_model:
                print("[Realtime] æ£€æµ‹åˆ°è§¦å‘è¯ï¼Œå¯åŠ¨å¤§æ¨¡å‹åˆ†æ...")
                self._update_status("ğŸ§  æ­£åœ¨æ·±åº¦åˆ†æ...", "processing")

                def run_big_model():
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    try:
                        loop.run_until_complete(self._process_with_big_model(user_message))
                    except Exception as e:
                        print(f"[å¤§æ¨¡å‹çº¿ç¨‹] é”™è¯¯: {e}")
                    finally:
                        loop.close()

                thread = threading.Thread(target=run_big_model, daemon=True)
                thread.start()
                print("[Realtime] å¤§æ¨¡å‹çº¿ç¨‹å·²å¯åŠ¨")

            self._update_status("âœ“ å®Œæˆ", "success")

        except Exception as e:
            self._update_status(f"âŒ é”™è¯¯: {str(e)}", "error")
            print(f"Realtimeæ¨¡å¼é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

    async def _async_llm_with_text_tts(self, user_message: str):
        """ä¼ ç»Ÿæ¨¡å¼ï¼šLLMç”Ÿæˆæ–‡æœ¬ â†’ TTSè½¬éŸ³é¢‘"""
        try:
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
            self.conversation_history.append({
                "role": "user",
                "content": user_message
            })

            # 1. æ›´æ–°ç³»ç»Ÿæç¤ºï¼ˆå¦‚æœæœ‰ç­–ç•¥æ›´æ–°ï¼‰
            if self.enable_dual_model and self.dual_model_manager:
                enhanced_prompt = self.dual_model_manager.get_enhanced_system_prompt(
                    self.base_system_prompt
                )
                self.conversation_history[0]["content"] = enhanced_prompt

            # 2. å¼€å§‹æµå¼LLMç”Ÿæˆï¼ˆå°æ¨¡å‹ï¼‰
            self._update_status("ğŸ¤– AIæ€è€ƒä¸­...", "processing")

            stream = await self.client.chat.completions.create(
                model=self.small_model,
                messages=self.conversation_history,
                stream=True,
                temperature=0.7,
                max_tokens=300
            )

            # 3. æµå¼å¤„ç†ï¼šè¾¹ç”Ÿæˆè¾¹TTS
            full_response = ""
            current_chunk = ""
            audio_queue = []  # å­˜å‚¨å¾…æ’­æ”¾çš„éŸ³é¢‘æ–‡ä»¶ (æ ¼å¼: {"audio_file": str, "sentence": str, "index": int})
            should_trigger_big_model = False
            sentence_counter = 0  # å¥å­è®¡æ•°å™¨

            # å¯åŠ¨éŸ³é¢‘æ’­æ”¾åç¨‹
            play_task = asyncio.create_task(self._audio_player(audio_queue))

            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    token = chunk.choices[0].delta.content
                    current_chunk += token
                    full_response += token

                    # æ£€æµ‹æ˜¯å¦æœ‰å®Œæ•´çš„å¥å­/çŸ­è¯­
                    sentences = self._extract_complete_sentences(current_chunk)

                    if sentences:
                        for sentence in sentences:
                            sentence_counter += 1

                            # æµå¼æ›´æ–°æ˜¾ç¤ºï¼ˆæ¯ç”Ÿæˆä¸€ä¸ªå¥å­å°±æ˜¾ç¤ºï¼‰
                            self._on_ai_response_streaming(sentence)

                            # æ£€æµ‹æ˜¯å¦è§¦å‘å¤§æ¨¡å‹
                            if (self.enable_dual_model and
                                self.dual_model_manager and
                                self.dual_model_manager.check_if_trigger_big_model(sentence)):
                                should_trigger_big_model = True

                            # é€šçŸ¥TTSå¼€å§‹è½¬æ¢
                            self._on_tts_progress(sentence, sentence_counter, "converting")

                            # ç«‹å³è¿›è¡ŒTTSï¼ˆä¼ å…¥å¥å­ç´¢å¼•ï¼‰
                            audio_file = await self._quick_tts(sentence, sentence_index=sentence_counter)
                            if audio_file:
                                # é€šçŸ¥TTSè½¬æ¢å®Œæˆï¼Œç­‰å¾…æ’­æ”¾
                                self._on_tts_progress(sentence, sentence_counter, "queued")
                                # å°†éŸ³é¢‘æ–‡ä»¶å’Œå¥å­ä¿¡æ¯ä¸€èµ·å…¥é˜Ÿ
                                audio_queue.append({
                                    "audio_file": audio_file,
                                    "sentence": sentence,
                                    "index": sentence_counter
                                })

                                # é¦–æ¬¡æ’­æ”¾æ—¶æ›´æ–°çŠ¶æ€
                                if len(audio_queue) == 1:
                                    self._update_status("ğŸ”Š æ’­æ”¾AIè¯­éŸ³...", "speaking")

                        # é‡ç½®ç¼“å†²åŒºï¼ˆä¿ç•™æœªå®Œæˆçš„éƒ¨åˆ†ï¼‰
                        current_chunk = self._get_remaining_text(current_chunk, sentences)

            # å¤„ç†æœ€åå‰©ä½™çš„æ–‡æœ¬
            if current_chunk.strip():
                sentence_counter += 1
                self._on_ai_response_streaming(current_chunk.strip())

                # é€šçŸ¥TTSå¼€å§‹è½¬æ¢
                self._on_tts_progress(current_chunk.strip(), sentence_counter, "converting")

                audio_file = await self._quick_tts(current_chunk.strip(), sentence_index=sentence_counter)
                if audio_file:
                    # é€šçŸ¥TTSè½¬æ¢å®Œæˆ
                    self._on_tts_progress(current_chunk.strip(), sentence_counter, "queued")
                    # å°†éŸ³é¢‘æ–‡ä»¶å’Œå¥å­ä¿¡æ¯ä¸€èµ·å…¥é˜Ÿ
                    audio_queue.append({
                        "audio_file": audio_file,
                        "sentence": current_chunk.strip(),
                        "index": sentence_counter
                    })

            # æ ‡è®°éŸ³é¢‘é˜Ÿåˆ—ç»“æŸ
            audio_queue.append(None)  # ç»“æŸä¿¡å·

            # ç­‰å¾…æ‰€æœ‰éŸ³é¢‘æ’­æ”¾å®Œæˆ
            await play_task

            # æ·»åŠ AIå›å¤åˆ°å†å²
            self.conversation_history.append({
                "role": "assistant",
                "content": full_response
            })

            # 4. å¦‚æœè§¦å‘äº†å¤§æ¨¡å‹ï¼Œåå°å¼‚æ­¥å¤„ç†
            if should_trigger_big_model:
                print("[è¯­éŸ³å¼•æ“] æ£€æµ‹åˆ°è§¦å‘è¯ï¼Œå¯åŠ¨å¤§æ¨¡å‹åˆ†æ...")
                self._update_status("ğŸ§  æ­£åœ¨æ·±åº¦åˆ†æ...", "processing")

                # åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œå¤§æ¨¡å‹ï¼ˆé¿å…event loopè¿‡æ—©ç»“æŸï¼‰
                def run_big_model():
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    try:
                        loop.run_until_complete(self._process_with_big_model(user_message))
                    except Exception as e:
                        print(f"[å¤§æ¨¡å‹çº¿ç¨‹] é”™è¯¯: {e}")
                    finally:
                        loop.close()

                thread = threading.Thread(target=run_big_model, daemon=True)
                thread.start()
                print("[è¯­éŸ³å¼•æ“] å¤§æ¨¡å‹çº¿ç¨‹å·²å¯åŠ¨")

            # å›è°ƒæ˜¾ç¤ºå®Œæ•´å›å¤
            self._on_ai_response(full_response.strip())

            self._update_status("âœ“ å®Œæˆ", "success")

        except Exception as e:
            self._update_status(f"âŒ é”™è¯¯: {str(e)}", "error")
            print(f"æµå¼LLM+TTSé”™è¯¯: {e}")

    async def _process_with_big_model(self, user_question: str):
        """åå°ä½¿ç”¨å¤§æ¨¡å‹å¤„ç†å¤æ‚é—®é¢˜"""
        try:
            result = await self.dual_model_manager.process_with_big_model(
                user_question,
                self.conversation_history,
                self.background_data,
                self.personal_memo
            )

            # å¤§æ¨¡å‹çš„ç­”æ¡ˆ
            big_model_answer = result["answer"]

            # æµå¼æ’­æ”¾å¤§æ¨¡å‹çš„å›å¤
            self._update_status("ğŸ“ ä¸“å®¶å›å¤ä¸­...", "speaking")

            # æ·»åŠ ä¸€ä¸ªæ ‡è¯†å‰ç¼€
            prefix = "æ ¹æ®è¯¦ç»†åˆ†æï¼Œ"
            self._on_ai_response_streaming(prefix)
            audio_file = await self._quick_tts(prefix)
            if audio_file:
                await self._play_single_audio(audio_file)

            # åˆ†å¥æ’­æ”¾å¤§æ¨¡å‹å›å¤
            sentences = self._extract_complete_sentences(big_model_answer)
            for sentence in sentences:
                self._on_ai_response_streaming(sentence)
                audio_file = await self._quick_tts(sentence)
                if audio_file:
                    await self._play_single_audio(audio_file)

            # æ·»åŠ å¤§æ¨¡å‹å›å¤åˆ°å†å²
            self.conversation_history.append({
                "role": "assistant",
                "content": prefix + big_model_answer
            })

            self._update_status("âœ“ ä¸“å®¶åˆ†æå®Œæˆ", "success")

        except Exception as e:
            import traceback
            print(f"[å¤§æ¨¡å‹å¤„ç†] é”™è¯¯: {e}")
            print(f"[å¤§æ¨¡å‹å¤„ç†] è¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}")
            self._update_status(f"âŒ å¤§æ¨¡å‹å¤„ç†å¤±è´¥: {str(e)}", "error")

    async def _play_single_audio(self, audio_file: str):
        """æ’­æ”¾å•ä¸ªéŸ³é¢‘æ–‡ä»¶ï¼ˆä¿ç•™æ–‡ä»¶ä¾›è°ƒè¯•ï¼‰"""
        if os.path.exists(audio_file):
            play_process = await asyncio.create_subprocess_exec(
                "afplay", audio_file,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            await play_process.communicate()
            # ä¸åˆ é™¤æ–‡ä»¶ï¼Œä¿ç•™ä¾›è°ƒè¯•
            # print(f"[TTSè°ƒè¯•] æ’­æ”¾å®Œæˆ: {os.path.basename(audio_file)}")

    def _extract_complete_sentences(self, text: str) -> list:
        """æå–å®Œæ•´çš„å¥å­ï¼ˆæŒ‰æ ‡ç‚¹ç¬¦å·åˆ†å‰²ï¼‰"""
        # åŒ¹é…ä¸­è‹±æ–‡æ ‡ç‚¹
        pattern = r'([^ï¼Œã€‚ï¼ï¼Ÿ,\.!?]+[ï¼Œã€‚ï¼ï¼Ÿ,\.!?]+)'
        matches = re.findall(pattern, text)
        return [m.strip() for m in matches if m.strip()]

    def _get_remaining_text(self, text: str, extracted_sentences: list) -> str:
        """è·å–æå–å¥å­åå‰©ä½™çš„æ–‡æœ¬"""
        for sentence in extracted_sentences:
            text = text.replace(sentence, '', 1)
        return text

    async def _trim_audio_end(self, input_path: str, trim_ms: int) -> Optional[str]:
        """
        è£å‰ªéŸ³é¢‘ç»“å°¾çš„é™éŸ³éƒ¨åˆ†

        Args:
            input_path: è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            trim_ms: è¦è£å‰ªçš„æ¯«ç§’æ•°

        Returns:
            è£å‰ªåçš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœæˆåŠŸï¼‰
        """
        try:
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
            base, ext = os.path.splitext(input_path)
            output_path = f"{base}_trimmed{ext}"

            # ä½¿ç”¨ffprobeè·å–éŸ³é¢‘æ—¶é•¿
            probe_process = await asyncio.create_subprocess_exec(
                "ffprobe",
                "-v", "error",
                "-show_entries", "format=duration",
                "-of", "default=noprint_wrappers=1:nokey=1",
                input_path,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, stderr = await probe_process.communicate()

            if probe_process.returncode != 0:
                print(f"[éŸ³é¢‘è£å‰ª] ffprobeå¤±è´¥: {stderr.decode()}")
                return input_path  # è¿”å›åŸå§‹æ–‡ä»¶

            # è§£ææ—¶é•¿
            duration_str = stdout.decode().strip()
            try:
                original_duration = float(duration_str)
            except ValueError:
                print(f"[éŸ³é¢‘è£å‰ª] æ— æ³•è§£ææ—¶é•¿: {duration_str}")
                return input_path

            # è®¡ç®—æ–°æ—¶é•¿ï¼ˆè£å‰ªç»“å°¾ï¼‰
            trim_seconds = trim_ms / 1000.0
            new_duration = original_duration - trim_seconds

            if new_duration <= 0:
                print(f"[éŸ³é¢‘è£å‰ª] éŸ³é¢‘å¤ªçŸ­ï¼Œæ— æ³•è£å‰ª ({original_duration}s < {trim_seconds}s)")
                return input_path

            # ä½¿ç”¨ffmpegè£å‰ªéŸ³é¢‘
            trim_process = await asyncio.create_subprocess_exec(
                "ffmpeg",
                "-y",  # è¦†ç›–è¾“å‡ºæ–‡ä»¶
                "-i", input_path,
                "-t", str(new_duration),  # è®¾ç½®æ–°æ—¶é•¿
                "-c", "copy",  # å¤åˆ¶ç¼–ç ï¼ˆå¿«é€Ÿï¼‰
                output_path,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, stderr = await trim_process.communicate()

            if trim_process.returncode == 0 and os.path.exists(output_path):
                # åˆ é™¤åŸå§‹æ–‡ä»¶
                os.unlink(input_path)
                print(f"[éŸ³é¢‘è£å‰ª] æˆåŠŸ: {original_duration:.3f}s -> {new_duration:.3f}s (è£å‰ª{trim_ms}ms)")
                return output_path
            else:
                print(f"[éŸ³é¢‘è£å‰ª] ffmpegå¤±è´¥: {stderr.decode()}")
                return input_path

        except Exception as e:
            print(f"[éŸ³é¢‘è£å‰ª] é”™è¯¯: {e}")
            return input_path  # è¿”å›åŸå§‹æ–‡ä»¶

    async def _quick_tts(self, text: str, sentence_index: int = 0) -> Optional[str]:
        """å¿«é€ŸTTSï¼ˆå•ä¸ªå¥å­/çŸ­è¯­ï¼‰- ä¿å­˜åˆ°æœ¬åœ°ä¾›è°ƒè¯•"""
        try:
            # ç”Ÿæˆæ–‡ä»¶åï¼šæ—¶é—´æˆ³ + å¥å­ç´¢å¼•
            timestamp = datetime.now().strftime("%H%M%S")

            if self.tts_engine == "local":
                # macOS sayå‘½ä»¤ï¼ˆæœ€å¿«ï¼‰
                filename = f"sentence_{sentence_index:03d}_{timestamp}.m4a"
                audio_path = os.path.join(self.tts_audio_dir, filename)

                process = await asyncio.create_subprocess_exec(
                    "say",
                    "-v", "Tingting",
                    "-o", audio_path,
                    "--data-format=LEF32@22050",
                    text,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                await process.communicate()

                if process.returncode == 0 and os.path.exists(audio_path):
                    print(f"[TTSè°ƒè¯•] å·²ä¿å­˜: {filename} | å†…å®¹: {text[:20]}...")

                    # è£å‰ªéŸ³é¢‘ç»“å°¾é™éŸ³
                    trimmed_path = await self._trim_audio_end(audio_path, self.trim_end_silence_ms)
                    return trimmed_path
                else:
                    return None

            elif self.tts_engine == "edge":
                # Edge TTS
                filename = f"sentence_{sentence_index:03d}_{timestamp}.mp3"
                audio_path = os.path.join(self.tts_audio_dir, filename)

                process = await asyncio.create_subprocess_exec(
                    "edge-tts",
                    "--voice", "zh-CN-XiaoxiaoNeural",
                    "--rate", "+10%",
                    "--pitch", "+5Hz",
                    "--text", text,
                    "--write-media", audio_path,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                await process.communicate()

                if process.returncode == 0 and os.path.exists(audio_path):
                    print(f"[TTSè°ƒè¯•] å·²ä¿å­˜: {filename} | å†…å®¹: {text[:20]}...")

                    # è£å‰ªéŸ³é¢‘ç»“å°¾é™éŸ³
                    trimmed_path = await self._trim_audio_end(audio_path, self.trim_end_silence_ms)
                    return trimmed_path
                else:
                    return None

            else:  # openai
                # OpenAI TTSï¼ˆè¾ƒæ…¢ï¼Œä¸æ¨èæµå¼ä½¿ç”¨ï¼‰
                response = await self.client.audio.speech.create(
                    model="tts-1",
                    voice="nova",
                    input=text
                )

                filename = f"sentence_{sentence_index:03d}_{timestamp}.mp3"
                audio_path = os.path.join(self.tts_audio_dir, filename)

                with open(audio_path, 'wb') as f:
                    f.write(response.content)

                print(f"[TTSè°ƒè¯•] å·²ä¿å­˜: {filename} | å†…å®¹: {text[:20]}...")

                # è£å‰ªéŸ³é¢‘ç»“å°¾é™éŸ³
                trimmed_path = await self._trim_audio_end(audio_path, self.trim_end_silence_ms)
                return trimmed_path

        except Exception as e:
            print(f"å¿«é€ŸTTSé”™è¯¯: {e}")
            return None

    async def _audio_player(self, audio_queue: list):
        """éŸ³é¢‘æ’­æ”¾å™¨ï¼ˆå¹¶å‘æ’­æ”¾é˜Ÿåˆ—ä¸­çš„éŸ³é¢‘ï¼‰"""
        try:
            while True:
                # ç­‰å¾…é˜Ÿåˆ—ä¸­æœ‰éŸ³é¢‘ï¼ˆä¼˜åŒ–ï¼šå‡å°‘è½®è¯¢å»¶è¿Ÿåˆ°10msï¼‰
                while len(audio_queue) == 0:
                    await asyncio.sleep(0.01)

                # å–å‡ºéŸ³é¢‘é¡¹ï¼ˆå¯èƒ½æ˜¯å­—å…¸æˆ–Noneï¼‰
                audio_item = audio_queue.pop(0)

                # Noneè¡¨ç¤ºé˜Ÿåˆ—ç»“æŸ
                if audio_item is None:
                    break

                # é€šçŸ¥å¼€å§‹æ’­æ”¾
                if isinstance(audio_item, dict):
                    sentence = audio_item["sentence"]
                    index = audio_item["index"]
                    audio_file = audio_item["audio_file"]

                    # é€šçŸ¥UIï¼šæ­£åœ¨æ’­æ”¾
                    self._on_tts_progress(sentence, index, "playing")
                else:
                    # å…¼å®¹æ—§æ ¼å¼ï¼ˆå­—ç¬¦ä¸²ï¼‰
                    audio_file = audio_item

                # æ’­æ”¾éŸ³é¢‘
                await self._play_single_audio(audio_file)

        except Exception as e:
            print(f"éŸ³é¢‘æ’­æ”¾å™¨é”™è¯¯: {e}")

    async def _record_audio(self) -> Optional[np.ndarray]:
        """å½•éŸ³ï¼ˆç®€åŒ–ç‰ˆï¼Œè‡ªåŠ¨é™éŸ³æ£€æµ‹ï¼‰"""
        try:
            audio_chunks = []
            silence_start_time = None

            def audio_callback(indata, _frames, _time_info, status):
                if status:
                    print(f"å½•éŸ³çŠ¶æ€: {status}")
                audio_chunks.append(indata.copy())

            with sd.InputStream(
                samplerate=self.sample_rate,
                channels=1,
                dtype=np.float32,
                callback=audio_callback
            ):
                start_time = time.time()
                while time.time() - start_time < self.max_recording_duration:
                    await asyncio.sleep(0.1)

                    if len(audio_chunks) > 0:
                        recent_audio = np.concatenate(audio_chunks[-5:])
                        rms = np.sqrt(np.mean(recent_audio ** 2))

                        if rms < self.silence_duration_to_stop:
                            if silence_start_time is None:
                                silence_start_time = time.time()
                            elif time.time() - silence_start_time > self.silence_threshold:
                                break
                        else:
                            silence_start_time = None

            if len(audio_chunks) == 0:
                return None

            audio_data = np.concatenate(audio_chunks)
            return audio_data

        except Exception as e:
            print(f"å½•éŸ³é”™è¯¯: {e}")
            return None

    async def _speech_to_text(self, audio_data: np.ndarray) -> str:
        """è¯­éŸ³è½¬æ–‡å­—"""
        try:
            # ä¿å­˜ä¸ºä¸´æ—¶WAVæ–‡ä»¶
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")

            with wave.open(temp_file.name, 'wb') as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(self.sample_rate)
                audio_int16 = (audio_data * 32767).astype(np.int16)
                wf.writeframes(audio_int16.tobytes())

            # è°ƒç”¨Whisper API
            with open(temp_file.name, 'rb') as audio_file:
                transcription = await self.client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file,
                    language="zh"
                )

            os.unlink(temp_file.name)
            return transcription.text.strip()

        except Exception as e:
            print(f"STTé”™è¯¯: {e}")
            return ""
